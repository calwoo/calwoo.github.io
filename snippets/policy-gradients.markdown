---
title: Policy gradients
author: Calvin
---

**Note: this is a snippet, to be eventually incorporated into an actual post.**

Consider a policy $\pi_\theta$ parameterized by $\theta$. The policy takes a state (the observed state, at least, in our reinforcement learning situations) and returns usually a probability distribution over the possible actions we can take at that state. Here, we can take the policy function $\pi_\theta$ to be something like a neural network, or any function approximator. 

A basic question in RL is how to learn a policy from the environment and rewards that we recieve. A popular method is to instead approximate the "value" of each state (or state-action) $\hat{Q}(s, a)$ and then derive the policy via maximization,

$$ \pi^*(s) = \argmax_{a\in A(s)}\hat{Q}(s,a) $$

We will take a more direct method, in which we will aim to maximize the **expected return** 

$$ J(\pi_\theta)=\mathbf{E}_{\tau\sim\pi_\theta}[R(\tau)] $$

by directly performing gradient **ascent** on the parameter space

$$ \theta_{t+1} = \theta_t + \alpha\nabla_\theta J(\pi_\theta) $$

This is called a **policy gradient**, but in this form it is pretty uncomputable. For one, we are taking the gradient of an expectation, which may not the expectation of anything-- hence we are differentiating across something stochastic, which may require some reparameterization of the underlying computational graph needed for autodifferentiation.


### basic policy gradient

However, it turns out in this situation things get much simplier for us and we can breathe a bit easier. Recalling the definition of the expectation as an integral, and the formula for logarithmic differentiation

$$ p(\tau|\theta)\cdot\nabla\log{p(\tau|\theta)} = \nabla{p(\tau|\theta)} $$

we have

$$
\begin{align*}
\nabla_\theta J(\pi_\theta) &= \nabla_\theta\int_\tau p(\tau|\theta)R(\tau) \\
    &= \int_\tau p(\tau|\theta)\nabla_\theta\log{p(\tau|\theta)}R(\tau) \\
    &= \mathbf{E}_{\tau\sim\pi_\theta}\left[\nabla_\theta\log{p(\tau|\theta)}R(\tau)\right]
\end{align*}
$$

Noting that for a given trajectory $\tau=(s_0, a_0, s_1, a_1, s_2,...)$, we have

$$ \log{p(\tau|\theta)} = \log{p(s_0)} + \sum_{t=0}^T\left(\log{p(s_{t+1}|s_t,a_t)}+\log{\pi_\theta(a_t|s_t)}\right) $$

and so

$$ \nabla_\theta\log{p(\tau|\theta)} = \sum_{t=0}^T \nabla_\theta \log{\pi_\theta(a_t|s_t)} $$

which gives the policy gradient

$$ \nabla_\theta J(\pi_\theta) = \mathbf{E}_{\tau\sim\pi_\theta}\left[\sum_{t=0}^T \nabla_\theta \log{\pi_\theta(a_t|s_t)}R(\tau)\right] $$

The great thing about having an expectation now, is that we can approximate it using Monte Carlo: let $\mathcal{D}=\{\tau_i\}_{i=1,...,K}$ be a collection of trajectories generated by the agent's policy $\pi_\theta$, we compute an estimate of the policy gradient via

$$ \hat{g} = \frac{1}{|\mathcal{D}|}\sum_{\tau\in\mathcal{D}}\sum_{t=0}^T \nabla_\theta \log{\pi_\theta(a_t|s_t)}R(\tau) $$

Implementing this in code is fairly straightforward (here we are using Pytorch): let `policy` be any standard neural network with input being a tensor representing the observation space, mapping to a distribution representing $\pi_\theta(a|s)$,

```python
import torch

policy = nnet()
def compute_policy(
    policy: torch.nn.Module, 
    obs: torch.Tensor
) -> torch.distributions.Categorical:
    logits = policy(obs)
    return Categorical(logits=logits)

# an action can be sampled from the policy
def sample_action(obs: torch.Tensor) -> torch.Tensor:
    action_dis = compute_policy(obs)
    return action_dis.sample().item()
```

To compute the policy gradient, we *could* find the gradient using jacobian-vector products and the differentiation library directly, but then we would have to directly construct the policy gradient and perform the gradient (descent) ourselves (recall that in Pytorch, we usually construct a scalar value that we directly find the gradient off using the `.backward()` method, that loads the gradient with respect to each parameter in the `Tensor` objects themselves).

Another approach is to construct a proxy value whose gradient is the policy gradient we seek. **Remember**, we can't just use the value $J(\pi_\theta)$ directly-- we tried above to avoid differentiating this value directly!

Let $\mathcal{D}$ be the above collection of sampled trajectories (rollouts) from the policy $\pi_\theta$. Consider the value

$$ \hat{\ell} = \frac{1}{|\mathcal{D}|}\sum_{\tau\in\mathcal{D}}\sum_{t=0}^T \log{\pi_\theta(a_t|s_t)}R(\tau) $$

Then $\nabla_\theta\hat{\ell} = \hat{g}$, the estimated policy gradient! So we instead take this scalar as our "loss" value and perform autodifferentiation on this tensor

```python
def compute_loss(observations, actions, rewards):
    logp = compute_policy(observations).log_prob(actions)
    return -(logp * rewards).sum(dim=-1).mean(dim=0)
```

Note that have a minus sign because we are *minimizing* the loss.


### baselines

We note a small mathematical lemma: note that

$$
\begin{align*}
    \mathbf{E}_{a\sim\pi_\theta}[\nabla_\theta\log{\pi_\theta(a|s)}] &= \int_{a\in A(s)} \pi_\theta(a|s)\nabla_\theta\log{\pi_\theta(a|s)} \\
        &= \int_{a\in A(s)}\nabla_\theta\pi_\theta(a|s) \\
        &= \nabla_\theta\int_{a\in A(s)}\pi_\theta(a|s) \\
        &= \nabla_\theta 1 \\
        &= 0
\end{align*}
$$

As a corollary, given any function of the observed state $s$ that **does not** depend on the action taken, $b(s)$, we have

$$ \mathbf{E}_{a\sim\pi_\theta}[\nabla_\theta\log{\pi_\theta(a|s)}b(s)] = 0 $$

We call such a function $b$ a **baseline**.

The purpose of baselines is to reduce the variance of the policy gradient, while preserving the unbiased nature of the Monte Carlo estimate. A usual baseline to take is the **on-policy value function**

$$ V^{\pi_\theta}(s) = \mathbf{E}_{\tau\sim\pi_\theta}[R(\theta)|s_0=s] $$


### actor-critic method

Consider the **on-policy Q-function**

$$ Q^{\pi_\theta}(s,a) = \mathbf{E}_{\tau\sim\pi_\theta}[R(\theta)|s_0=s,a_0=a] $$

We can take this value as the basis of our policy gradients by showing that

$$ \nabla_\theta J(\pi_\theta) = \mathbf{E}_{\tau\sim\pi_\theta}\left[\sum_{t=0}^T \nabla_\theta \log{\pi_\theta(a_t|s_t)}Q^{\pi_\theta}(s_t,a_t)\right] $$

The proof can be found [here](https://spinningup.openai.com/en/latest/spinningup/extra_pg_proof2.html). Recalling that the state function $V^{\pi_\theta}(s)$ can be taken as a baseline, we conclude that the policy gradient can be described as

$$ \nabla_\theta J(\pi_\theta) = \mathbf{E}_{\tau\sim\pi_\theta}\left[\sum_{t=0}^T \nabla_\theta \log{\pi_\theta(a_t|s_t)}\left(Q^{\pi_\theta}(s_t,a_t)-V^{\pi_\theta}(s_t)\right)\right] $$

where $A^{\pi_\theta}(s_t, a_t) = Q^{\pi_\theta}(s_t,a_t)-V^{\pi_\theta}(s_t)$ is known as the **advantage** of taking action $a_t$ at state $s_t$ (this looks like regret)! As an estimate,

$$ \hat{g} = \frac{1}{|\mathcal{D}|}\sum_{\tau\in\mathcal{D}}\sum_{t=0}^T \nabla_\theta \log{\pi_\theta(a_t|s_t)}\left(Q^{\pi_\theta}(s_t,a_t)-V^{\pi_\theta}(s_t)\right) $$

is significantly lower variance than the original estimate, and is the form we will take for most policy gradient situations.

We are then left with the question of computing the advantage. Let $r_t$ be the reward given by the environment when applying action $a_t$ to state $s_t$. Then note that

$$ Q^{\pi_\theta}(s_t, a_t) = r_t + \gamma\max_{a\in A(s_{t+1})} Q^{\pi_\theta}(s_{t+1}, a) = r_t + \gamma V^{\pi_\theta}(s_{t+1}) $$

where $\gamma$ is the discount factor used to compute $R(\tau)$. In particular, we can compute the advantage as

$$ A^{\pi_\theta}(s_t, a_t) = r_t + \gamma V^{\pi_\theta}(s_{t+1}) - V^{\pi_\theta}(s_t) $$

This reduces our burden to computing $V^{\pi_\theta}$. Technically this can be computed via Monte Carlo (or TD-$n$) rollouts, but in many situations we can train a neural network $V_\phi(s_t)$ concurrently with the policy $\pi_\theta(a_t|s_t)$ and use that as an approximation to the above advantage calculation. Our **critic** network $V_\phi(s_t)$ is trained to minimize the MSE between itself and the expected rewards from rollouts generated by the **actor** policy $\pi_\theta$,

$$ \phi_{t+1} = \argmin_{\phi} \mathbf{E}_{s_t,\tau\sim\pi_{\theta_t}}\left[\left(V_{\phi_t}(s_t)-\hat{R}(\tau)\right)^2\right] $$

This is the **advantage actor-critic method** (A2C).

Briefly, in Python:

```python
import torch.nn.functional as F
actor_policy_net = nnet()
critic_net = nnet()

opt_actor = torch.optim.Adam(actor_policy_net.parameters())
opt_critic = torch.optim.Adam(critic_net.parameters())

def train_actor_critic(obss, acts, rewards):
    # train critic
    opt_critic.zero_grad()
    values = critic_net(obss).squeeze(dim=1)
    vf_loss = F.mse_loss(values, rewards, reduction="sum")
    vf_loss.backward()

    # train actor policy
    opt_actor.zero_grad()
    with torch.no_grad():
        values_policy = critic_net(obss)
    advantages = rewards - values
    logits = actor_policy_net(obss)
    logp = -F.cross_entropy(logits, acts, reduction="none")
    pi_loss = -(logp * advantages).sum()
    pi_loss.backward()

    opt_actor.step()
    opt_critic.step()
```
