<!doctype html>
<html lang="en">
    <head>
        <link rel="icon" type="image/x-icon" href="../favicon.ico" />
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Probabilistic effects programming with Pyro - Calvin Woo's blog</title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../css/syntax.css" />
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG" crossorigin="anonymous">
    </head>
    <body>
        <header>
            <div class="logo">
                <a href="../">Calvin's Notebook</a>
            </div>
            <nav>
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../archive.html">Archive</a>
            </nav>
        </header>

        <main role="main">
            <h1>Probabilistic effects programming with Pyro</h1>
            <article>
    <section class="header">
        Posted on July 30, 2020
        
            by Calvin
        
    </section>
    <section>
        <p><strong>Note: This blog post is still a rough draft. Read on with caution.</strong></p>
<p>A lot of machine learning that we do is probabilistic, even though we don’t phrase it as such. Indeed, one can think of regularized linear regression as a hierarchical Bayesian model where the outcomes are normally-distributed with a fixed (yet unknown) variance. More often than not, thinking in terms of the probabilistic perspective leads us to generalizations of our favorite algorithms. As a trite example, if we wanted a robust version of the above regression, we might replace our normal distribution with a distribution with heavier tails, such as the Student <span class="math inline">\(t\)</span>-distribution.</p>
<p>However most of the time, we ignore this probabilistic bounty and resort to maximal likelihood techniques (or if we end up being fancy, MAP!). By resorting to <em>averages</em> we throw away a huge amount of relevant information, and drive us away from principled <a href="https://en.wikipedia.org/wiki/James%E2%80%93Stein_estimator">optimality</a>. Fully exploiting this leads us to the Bayesian framework, which is characterized by computational complexity and inferential madness. Is there a way to just write models and have the hard computations done for you automatically?</p>
<h3 id="probabilistic-programming">probabilistic programming</h3>
<p>Enter probabilistic programming. PPLs have for years been promising data scientists a beautiful story– write your models, press a button, and off you go! There have been bumps along the way. Early probabilistic programming languages were like early <code>tensorflow</code>: models were constructed in an embedded DSL with huge limitations as to what you could do, since each new control flow feature had to be implemented in the confines of the language. Take for instance a generative story for a geometric distribution <span class="math inline">\(x\sim\text{Geom}(p)\)</span>,</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"><span class="kw">def</span> geometric(p):</a>
<a class="sourceLine" id="cb1-2" title="2">    b <span class="op">=</span> flip(p) <span class="co"># bernoulli with param p</span></a>
<a class="sourceLine" id="cb1-3" title="3">    <span class="cf">if</span> <span class="bu">bool</span>(b):</a>
<a class="sourceLine" id="cb1-4" title="4">        <span class="cf">return</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb1-5" title="5">    <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb1-6" title="6">        <span class="cf">return</span> <span class="dv">1</span> <span class="op">+</span> geometric(p)</a></code></pre></div>
<p>This simple distribution has very complicated control flow! Inference on this model requires traversing computational graphs of variable size, so we need PPLs that can deal with this and other kinds of control flow in a natural way.</p>
<p>I’ve been an advocate of the control flow implemented via <strong>algebraic effects</strong> systems, where computations are given effectful emissions, which are captured and <em>handled</em> by outside effect handlers. While many (functional) languages deal with computational effects by passing them around in monadic types, algebraic effect languages use bounding-constructs like delimited continuations to restrict the scope of effects.</p>
<p>It turns out this strategy can have a lot of mileage, as it is the main idea behind the construction of Uber’s probabilistic programming language <a href="https://pyro.ai/">Pyro</a>. While the full strength of delimited continuations is difficult to replicate in Python, one-shot continuations are easier to implement by passing global state around. To see examples of full delimited continuations, I have some blog posts dedicated to it.</p>
<h3 id="minipyro-annotated">minipyro, annotated</h3>
<p>In this post I want to try and walk through the implementation of Pyro, starting with its main effects system. This is basically an annotated form of <a href="https://pyro.ai/examples/minipyro.html">minipyro</a>. I think it is an interesting case study in how future PPLs can be build in general-purpose languages, and understanding how Pyro works under the hood is a great way to think about its limitations, and how to potentially get around it (for example, combining with distributed training in <code>pytorch</code> is tricky, but not impossible).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="im">import</span> weakref</a>
<a class="sourceLine" id="cb2-2" title="2"><span class="im">from</span> collections <span class="im">import</span> OrderedDict</a>
<a class="sourceLine" id="cb2-3" title="3"></a>
<a class="sourceLine" id="cb2-4" title="4"><span class="im">import</span> torch</a></code></pre></div>
<p><code>pyro</code> is a probabilistic programming language, and as such are built with random variables and distributions as primitive objects. To perform inference on data, distributions expose an API of two functions– <code>sample</code> and <code>log_prob</code>.</p>
<p>Simply, <code>sample</code> samples a tensor from the given distribution, and <code>log_prob</code> returns the log-probability of a tensor computed from the pdf or pmf of the distribution. This, along with gradients of the log-probability (which is why <code>pyro</code> is built with the auto-diff properties of <code>pytorch</code>) is enough to allow for the sampling of the posterior distribution from probabilistic models, which is what automated inference is designed for.</p>
<p>However, we want our inference to run automatically in the background, without the need for us to write them ourselves. This is fairly easy if all we are doing is running, say, MCMC. We run our model repeatedly, sampling from each distribution and computing log-probabilities, taking gradients and running acceptance protocols. This can be done with a giant for loop. So why does <code>pyro</code> make it so complicated with <strong>effect handlers</strong>?</p>
<p>Well, its because we often want to remember the entire <em>trace</em> of a probabilistic program. For example, suppose we have a model and want to understand what happens when we condition on a parameter taking on a certain value? We can use a giant loop with rejection sampling semantics, but we’re gonna be running this for a long time before any convergence can occur. A better idea is to <em>start in the middle</em> and rerun simulations from the middle of a sampling process. To do this in the background, we need to keep these <em>effects</em> hidden from us, using <em>effect handlers</em>.</p>
<p>First things first, we need to keep track of our two kinds of global state: <code>effect handlers</code> and <code>named parameters</code></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1">PYRO_STACK <span class="op">=</span> []</a>
<a class="sourceLine" id="cb3-2" title="2">PARAM_STORE <span class="op">=</span> {}  <span class="co"># maps name -&gt; (unconstrained_value, constraint)</span></a>
<a class="sourceLine" id="cb3-3" title="3"></a>
<a class="sourceLine" id="cb3-4" title="4"><span class="kw">def</span> get_param_store():</a>
<a class="sourceLine" id="cb3-5" title="5">    <span class="cf">return</span> PARAM_STORE</a></code></pre></div>
<p><code>pyro</code> uses the <code>Messenger</code> class as the main abstraction of an effect handler. Okay, what the heck is an effect? a <strong>computational effect</strong> can be thought of as any unpure artifact of a computation. Intuitively, we can try and pretend python is a functional programming language and an effects system is something that is “building something in the background” as a computation is executed, e.g. building a computational graph or recording the sample/log-probs as a probabilistic model is evaluated.</p>
<p>To capture the idea of a computation entering an “effectful environment” we use python <em>context managers</em>. Such handlers record all the effects being emitted during a computation and <em>handle</em> them. Multiple effect handlers can wrap a single computation, and effects emitted are bubbled up through the handlers sequentially until they are captured by a relevant one.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1"><span class="kw">class</span> Messenger:</a>
<a class="sourceLine" id="cb4-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, fn<span class="op">=</span><span class="va">None</span>):</a>
<a class="sourceLine" id="cb4-3" title="3">        <span class="co"># effect handlers wrap a computation, here described as a function</span></a>
<a class="sourceLine" id="cb4-4" title="4">        <span class="va">self</span>.fn <span class="op">=</span> fn</a>
<a class="sourceLine" id="cb4-5" title="5">        </a>
<a class="sourceLine" id="cb4-6" title="6">    <span class="kw">def</span> <span class="fu">__enter__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb4-7" title="7">        <span class="co"># make the environment aware of the handler's existence</span></a>
<a class="sourceLine" id="cb4-8" title="8">        PYRO_STACK.append(<span class="va">self</span>)</a>
<a class="sourceLine" id="cb4-9" title="9">        </a>
<a class="sourceLine" id="cb4-10" title="10">    <span class="kw">def</span> <span class="fu">__exit__</span>(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</a>
<a class="sourceLine" id="cb4-11" title="11">        <span class="co"># exiting a handler removes it from the stack</span></a>
<a class="sourceLine" id="cb4-12" title="12">        <span class="cf">assert</span> PYRO_STACK[<span class="op">-</span><span class="dv">1</span>] <span class="kw">is</span> <span class="va">self</span></a>
<a class="sourceLine" id="cb4-13" title="13">        PYRO_STACK.pop()</a>
<a class="sourceLine" id="cb4-14" title="14">        </a>
<a class="sourceLine" id="cb4-15" title="15">    <span class="co"># these will be explained later...</span></a>
<a class="sourceLine" id="cb4-16" title="16">    <span class="kw">def</span> process_message(<span class="va">self</span>, msg):</a>
<a class="sourceLine" id="cb4-17" title="17">        <span class="cf">pass</span></a>
<a class="sourceLine" id="cb4-18" title="18"></a>
<a class="sourceLine" id="cb4-19" title="19">    <span class="kw">def</span> postprocess_message(<span class="va">self</span>, msg):</a>
<a class="sourceLine" id="cb4-20" title="20">        <span class="cf">pass</span></a>
<a class="sourceLine" id="cb4-21" title="21"></a>
<a class="sourceLine" id="cb4-22" title="22">    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</a>
<a class="sourceLine" id="cb4-23" title="23">        <span class="co"># when a function is wrapped with an effect handler, every time it is called, we</span></a>
<a class="sourceLine" id="cb4-24" title="24">        <span class="co"># now allow it to emit effects to be captured by running it in the context of the</span></a>
<a class="sourceLine" id="cb4-25" title="25">        <span class="co"># effect handler</span></a>
<a class="sourceLine" id="cb4-26" title="26">        <span class="cf">with</span> <span class="va">self</span>:</a>
<a class="sourceLine" id="cb4-27" title="27">            <span class="cf">return</span> <span class="va">self</span>.fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</a></code></pre></div>
<p>The comments above are explanatory. an example of an effect handler (again, we think of it as “effectful augmentation of computations”) is given by <code>trace</code>. In a nutshell, <code>trace</code> records the inputs and outputs of a function and keeps that information around with other metadata.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1"><span class="kw">class</span> trace(Messenger):</a>
<a class="sourceLine" id="cb5-2" title="2">    <span class="kw">def</span> <span class="fu">__enter__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb5-3" title="3">        <span class="bu">super</span>(trace, <span class="va">self</span>).<span class="fu">__enter__</span>()</a>
<a class="sourceLine" id="cb5-4" title="4">        <span class="co"># augmented data is recorded as a python dict</span></a>
<a class="sourceLine" id="cb5-5" title="5">        <span class="va">self</span>.trace <span class="op">=</span> OrderedDict()</a>
<a class="sourceLine" id="cb5-6" title="6">        <span class="cf">return</span> <span class="va">self</span>.trace</a>
<a class="sourceLine" id="cb5-7" title="7">    </a>
<a class="sourceLine" id="cb5-8" title="8">    <span class="co">&quot;&quot;&quot;trace is intended to be the 'outer' effect handler in the stack,</span></a>
<a class="sourceLine" id="cb5-9" title="9"><span class="co">    and will record the value after all other effects have been applied. in</span></a>
<a class="sourceLine" id="cb5-10" title="10"><span class="co">    this sense, trace is our main bookkeeping device in our stochastic graphs&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb5-11" title="11">    <span class="kw">def</span> postprocess_message(<span class="va">self</span>, msg):</a>
<a class="sourceLine" id="cb5-12" title="12">        <span class="cf">assert</span> msg[<span class="st">&quot;type&quot;</span>] <span class="op">!=</span> <span class="st">&quot;sample&quot;</span> <span class="kw">or</span> msg[<span class="st">&quot;name&quot;</span>] <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.trace, <span class="op">\</span></a>
<a class="sourceLine" id="cb5-13" title="13">            <span class="co">&quot;sample sites must have unique names&quot;</span></a>
<a class="sourceLine" id="cb5-14" title="14">        <span class="va">self</span>.trace[msg[<span class="st">&quot;name&quot;</span>]] <span class="op">=</span> msg.copy()</a>
<a class="sourceLine" id="cb5-15" title="15">        </a>
<a class="sourceLine" id="cb5-16" title="16">    <span class="kw">def</span> get_trace(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</a>
<a class="sourceLine" id="cb5-17" title="17">        <span class="co"># runs the underlying computation and returns the recorded trace</span></a>
<a class="sourceLine" id="cb5-18" title="18">        <span class="va">self</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</a>
<a class="sourceLine" id="cb5-19" title="19">        <span class="cf">return</span> <span class="va">self</span>.trace</a></code></pre></div>
<p>What is <code>msg</code> in the above? It’s our <strong>effect</strong>. In <code>pyro</code>’s implementation, effects are reified as messages (dicts) being passed around the context managers, which are processed/post-processed.</p>
<p>Before we see more handlers, let’s implement the running of an effectful computation and elucidate how messages are passed around.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" title="1"><span class="kw">def</span> apply_stack(msg):</a>
<a class="sourceLine" id="cb6-2" title="2">    <span class="co">&quot;&quot;&quot;applies the effect handler stack to an effectful computation</span></a>
<a class="sourceLine" id="cb6-3" title="3"><span class="co">    </span></a>
<a class="sourceLine" id="cb6-4" title="4"><span class="co">    the scheme is as follows: something like pyro.sample or pyro.param</span></a>
<a class="sourceLine" id="cb6-5" title="5"><span class="co">    initializes a msg (an effect) which is then passed through each</span></a>
<a class="sourceLine" id="cb6-6" title="6"><span class="co">    handler in the stack.</span></a>
<a class="sourceLine" id="cb6-7" title="7"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb6-8" title="8">    </a>
<a class="sourceLine" id="cb6-9" title="9">    <span class="cf">for</span> i, handler <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">reversed</span>(PYRO_STACK)):</a>
<a class="sourceLine" id="cb6-10" title="10">        <span class="co"># process message</span></a>
<a class="sourceLine" id="cb6-11" title="11">        handler.process_message(msg)</a>
<a class="sourceLine" id="cb6-12" title="12">        <span class="co"># a stop message prematurely stops propagation</span></a>
<a class="sourceLine" id="cb6-13" title="13">        <span class="cf">if</span> msg.get(<span class="st">&quot;stop&quot;</span>):</a>
<a class="sourceLine" id="cb6-14" title="14">            <span class="cf">break</span></a>
<a class="sourceLine" id="cb6-15" title="15">            </a>
<a class="sourceLine" id="cb6-16" title="16">    <span class="co"># perform the (uneffectful) computation</span></a>
<a class="sourceLine" id="cb6-17" title="17">    <span class="cf">if</span> msg[<span class="st">&quot;value&quot;</span>] <span class="kw">is</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb6-18" title="18">        msg[<span class="st">&quot;value&quot;</span>] <span class="op">=</span> msg[<span class="st">&quot;fn&quot;</span>](<span class="op">*</span>msg[<span class="st">&quot;args&quot;</span>])</a>
<a class="sourceLine" id="cb6-19" title="19">        </a>
<a class="sourceLine" id="cb6-20" title="20">    <span class="co"># postprocess message through handlers in reverse order,</span></a>
<a class="sourceLine" id="cb6-21" title="21">    <span class="co"># keeping in mind that we might have stopped prematurely</span></a>
<a class="sourceLine" id="cb6-22" title="22">    <span class="cf">for</span> handler <span class="kw">in</span> PYRO_STACK[<span class="op">-</span>i<span class="dv">-1</span>:]:</a>
<a class="sourceLine" id="cb6-23" title="23">        handler.postprocess_message(msg)</a>
<a class="sourceLine" id="cb6-24" title="24">        </a>
<a class="sourceLine" id="cb6-25" title="25">    <span class="cf">return</span> msg</a></code></pre></div>
<p>So far, this is the core of the effect handling system in <code>pyro</code>! To see an example of this, let’s start with the core function of a probabilistic programming system: sampling from models. Given a distribution <code>d</code>, calling <code>d.sample()</code> is a uneffectful computation (ignoring randomness)– to make it emit an “effect message”, we wrap all primitive distributions in a <code>pyro.sample</code> handler to allow samples to have this effectful structure.</p>
<p><strong>note</strong>: This is not a <code>Messenger</code>! <code>pyro.sample</code> is intended as an effect emitter– it should by itself never handle any effects itself.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" title="1"><span class="kw">def</span> sample(name, fn, <span class="op">*</span>args, <span class="op">**</span>kwargs):</a>
<a class="sourceLine" id="cb7-2" title="2">    <span class="co"># if this site is observable (i.e. a place to be conditioned by data), we</span></a>
<a class="sourceLine" id="cb7-3" title="3">    <span class="co"># mark that directly</span></a>
<a class="sourceLine" id="cb7-4" title="4">    obs <span class="op">=</span> kwargs.pop(<span class="st">&quot;obs&quot;</span>, <span class="va">None</span>)</a>
<a class="sourceLine" id="cb7-5" title="5">    </a>
<a class="sourceLine" id="cb7-6" title="6">    <span class="co"># if there are no active Messengers, we just draw a sample and return it as expected</span></a>
<a class="sourceLine" id="cb7-7" title="7">    <span class="cf">if</span> <span class="kw">not</span> PYRO_STACK:</a>
<a class="sourceLine" id="cb7-8" title="8">        <span class="cf">return</span> fn(<span class="op">*</span>args, <span class="op">**</span>kwargs)</a>
<a class="sourceLine" id="cb7-9" title="9">    </a>
<a class="sourceLine" id="cb7-10" title="10">    <span class="co"># else, an effect (msg) is initialized</span></a>
<a class="sourceLine" id="cb7-11" title="11">    effect_msg <span class="op">=</span> {</a>
<a class="sourceLine" id="cb7-12" title="12">        <span class="st">&quot;type&quot;</span>: <span class="st">&quot;sample&quot;</span>,</a>
<a class="sourceLine" id="cb7-13" title="13">        <span class="st">&quot;name&quot;</span>: name,     <span class="co"># sampling sites are given unique names</span></a>
<a class="sourceLine" id="cb7-14" title="14">        <span class="st">&quot;fn&quot;</span>: fn,</a>
<a class="sourceLine" id="cb7-15" title="15">        <span class="st">&quot;args&quot;</span>: args,</a>
<a class="sourceLine" id="cb7-16" title="16">        <span class="st">&quot;kwargs&quot;</span>: kwargs,</a>
<a class="sourceLine" id="cb7-17" title="17">        <span class="st">&quot;value&quot;</span>: obs}</a>
<a class="sourceLine" id="cb7-18" title="18">    </a>
<a class="sourceLine" id="cb7-19" title="19">    <span class="co"># pass it through the effect handlers</span></a>
<a class="sourceLine" id="cb7-20" title="20">    processed_effect_msg <span class="op">=</span> apply_stack(effect_msg)</a>
<a class="sourceLine" id="cb7-21" title="21">    <span class="cf">return</span> processed_effect_msg[<span class="st">&quot;value&quot;</span>]</a></code></pre></div>
<p>To keep things straight in our heads, let’s run through a basic example.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" title="1"><span class="im">import</span> pyro.distributions <span class="im">as</span> dist</a>
<a class="sourceLine" id="cb8-2" title="2"></a>
<a class="sourceLine" id="cb8-3" title="3">sample(<span class="st">&quot;test&quot;</span>, dist.Normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>))</a>
<a class="sourceLine" id="cb8-4" title="4"><span class="op">&gt;</span> tensor(<span class="fl">0.0009</span>)</a></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" title="1"><span class="co"># a simple probabilistic model is merely a generative data story</span></a>
<a class="sourceLine" id="cb9-2" title="2"><span class="kw">def</span> simple_model():</a>
<a class="sourceLine" id="cb9-3" title="3">    switch <span class="op">=</span> sample(<span class="st">&quot;test_bool&quot;</span>, dist.Bernoulli(<span class="fl">0.5</span>))</a>
<a class="sourceLine" id="cb9-4" title="4">    <span class="cf">if</span> switch:</a>
<a class="sourceLine" id="cb9-5" title="5">        a <span class="op">=</span> sample(<span class="st">&quot;test_a&quot;</span>, dist.Normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>))</a>
<a class="sourceLine" id="cb9-6" title="6">    <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb9-7" title="7">        b <span class="op">=</span> sample(<span class="st">&quot;test_b&quot;</span>, dist.Normal(loc<span class="op">=</span><span class="dv">1</span>, scale<span class="op">=</span><span class="fl">0.5</span>))</a>
<a class="sourceLine" id="cb9-8" title="8">        a <span class="op">=</span> sample(<span class="st">&quot;test_a&quot;</span>, dist.Normal(loc<span class="op">=</span>b, scale<span class="op">=</span><span class="dv">1</span>))</a>
<a class="sourceLine" id="cb9-9" title="9">    <span class="cf">return</span> a</a>
<a class="sourceLine" id="cb9-10" title="10"></a>
<a class="sourceLine" id="cb9-11" title="11">sample_trace <span class="op">=</span> trace(simple_model).get_trace()</a>
<a class="sourceLine" id="cb9-12" title="12">sample_trace</a>
<a class="sourceLine" id="cb9-13" title="13"><span class="op">&gt;</span> OrderedDict([(<span class="st">'test_bool'</span>,</a>
<a class="sourceLine" id="cb9-14" title="14"><span class="op">&gt;</span>               {<span class="st">'type'</span>: <span class="st">'sample'</span>,</a>
<a class="sourceLine" id="cb9-15" title="15"><span class="op">&gt;</span>                <span class="st">'name'</span>: <span class="st">'test_bool'</span>,</a>
<a class="sourceLine" id="cb9-16" title="16"><span class="op">&gt;</span>                <span class="st">'fn'</span>: Bernoulli(probs: <span class="fl">0.5</span>),</a>
<a class="sourceLine" id="cb9-17" title="17"><span class="op">&gt;</span>                <span class="st">'args'</span>: (),</a>
<a class="sourceLine" id="cb9-18" title="18"><span class="op">&gt;</span>                <span class="st">'kwargs'</span>: {},</a>
<a class="sourceLine" id="cb9-19" title="19"><span class="op">&gt;</span>                <span class="st">'value'</span>: tensor(<span class="fl">1.</span>)}),</a>
<a class="sourceLine" id="cb9-20" title="20"><span class="op">&gt;</span>              (<span class="st">'test_a'</span>,</a>
<a class="sourceLine" id="cb9-21" title="21"><span class="op">&gt;</span>               {<span class="st">'type'</span>: <span class="st">'sample'</span>,</a>
<a class="sourceLine" id="cb9-22" title="22"><span class="op">&gt;</span>                <span class="st">'name'</span>: <span class="st">'test_a'</span>,</a>
<a class="sourceLine" id="cb9-23" title="23"><span class="op">&gt;</span>                <span class="st">'fn'</span>: Normal(loc: <span class="fl">0.0</span>, scale: <span class="fl">1.0</span>),</a>
<a class="sourceLine" id="cb9-24" title="24"><span class="op">&gt;</span>                <span class="st">'args'</span>: (),</a>
<a class="sourceLine" id="cb9-25" title="25"><span class="op">&gt;</span>                <span class="st">'kwargs'</span>: {},</a>
<a class="sourceLine" id="cb9-26" title="26"><span class="op">&gt;</span>                <span class="st">'value'</span>: tensor(<span class="fl">1.0179</span>)})])</a></code></pre></div>
<p>This is a single trace of the probabilistic model <code>simple_model</code> above.</p>
<p>Now that we can capture sample traces from a model, what can we do with it? We can use it to answer probabilistic queries like, <strong>what is the joint probability of the sample</strong>? This can be given by summing up the log-probabilities of each sample from the sites and exponentiating it.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" title="1"><span class="kw">def</span> log_prob_sum(tr):</a>
<a class="sourceLine" id="cb10-2" title="2">    log_p_sum <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb10-3" title="3">    <span class="cf">for</span> site_name, site_msg <span class="kw">in</span> tr.items():</a>
<a class="sourceLine" id="cb10-4" title="4">        log_prob <span class="op">=</span> site_msg[<span class="st">&quot;fn&quot;</span>].log_prob(site_msg[<span class="st">&quot;value&quot;</span>])</a>
<a class="sourceLine" id="cb10-5" title="5">        log_p_sum <span class="op">+=</span> log_prob</a>
<a class="sourceLine" id="cb10-6" title="6">    <span class="cf">return</span> log_p_sum</a>
<a class="sourceLine" id="cb10-7" title="7"></a>
<a class="sourceLine" id="cb10-8" title="8">log_prob_sum(sample_trace).exp()</a>
<a class="sourceLine" id="cb10-9" title="9"><span class="op">&gt;</span> tensor(<span class="fl">0.1188</span>)</a></code></pre></div>
<p>Let’s bump the complexity of our probabilistic models up a bit. Recall that <strong>plate notation</strong> is a method of representing variables that repeat in a graphical model. For example, we might wish to model <code>N</code> normally distributed data points where the mean is itself normally-distributed:</p>
<p><span class="math display">\[ x_i \sim \text{Normal}(\alpha, 1) \text{ for }i=1,...,N \]</span> <span class="math display">\[ \alpha \sim \text{Normal}(0, 1) \]</span></p>
<p>To represent this without loops, we treat it as an effect handler.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" title="1"><span class="kw">class</span> plate(Messenger):</a>
<a class="sourceLine" id="cb11-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, name, size, dim<span class="op">=</span><span class="va">None</span>):</a>
<a class="sourceLine" id="cb11-3" title="3">        <span class="cf">if</span> dim <span class="kw">is</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb11-4" title="4">            <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">&quot;plate requires a dim arg&quot;</span>)</a>
<a class="sourceLine" id="cb11-5" title="5">        <span class="co"># in this plate, we only implement broadcasting semantics</span></a>
<a class="sourceLine" id="cb11-6" title="6">        <span class="cf">assert</span> dim <span class="op">&lt;</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb11-7" title="7">        </a>
<a class="sourceLine" id="cb11-8" title="8">        <span class="va">self</span>.size <span class="op">=</span> size</a>
<a class="sourceLine" id="cb11-9" title="9">        <span class="va">self</span>.dim <span class="op">=</span> dim</a>
<a class="sourceLine" id="cb11-10" title="10">        <span class="bu">super</span>(plate, <span class="va">self</span>).<span class="fu">__init__</span>(fn<span class="op">=</span><span class="va">None</span>)</a>
<a class="sourceLine" id="cb11-11" title="11">        </a>
<a class="sourceLine" id="cb11-12" title="12">    <span class="kw">def</span> process_message(<span class="va">self</span>, msg):</a>
<a class="sourceLine" id="cb11-13" title="13">        <span class="co"># plates must wrap over pyro.sample</span></a>
<a class="sourceLine" id="cb11-14" title="14">        <span class="cf">if</span> msg[<span class="st">&quot;type&quot;</span>] <span class="op">==</span> <span class="st">&quot;sample&quot;</span>:</a>
<a class="sourceLine" id="cb11-15" title="15">            <span class="co"># trivially expand batch_shape of underlying distribution</span></a>
<a class="sourceLine" id="cb11-16" title="16">            batch_shape <span class="op">=</span> msg[<span class="st">&quot;fn&quot;</span>].batch_shape</a>
<a class="sourceLine" id="cb11-17" title="17">            <span class="cf">if</span> <span class="bu">len</span>(batch_shape) <span class="op">&lt;</span> <span class="op">-</span><span class="va">self</span>.dim <span class="kw">or</span> batch_shape[<span class="va">self</span>.dim] <span class="op">!=</span> <span class="va">self</span>.size:</a>
<a class="sourceLine" id="cb11-18" title="18">                batch_shape <span class="op">=</span> [<span class="dv">1</span>] <span class="op">*</span> (<span class="op">-</span><span class="va">self</span>.dim <span class="op">-</span> <span class="bu">len</span>(batch_shape)) <span class="op">+</span> <span class="bu">list</span>(batch_shape)</a>
<a class="sourceLine" id="cb11-19" title="19">                batch_shape[<span class="va">self</span>.dim] <span class="op">=</span> <span class="va">self</span>.size</a>
<a class="sourceLine" id="cb11-20" title="20">                msg[<span class="st">&quot;fn&quot;</span>] <span class="op">=</span> msg[<span class="st">&quot;fn&quot;</span>].expand(torch.Size(batch_shape))</a>
<a class="sourceLine" id="cb11-21" title="21">                </a>
<a class="sourceLine" id="cb11-22" title="22">    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>):</a>
<a class="sourceLine" id="cb11-23" title="23">        <span class="cf">return</span> <span class="bu">range</span>(<span class="va">self</span>.size)</a></code></pre></div>
<p>Now the model above is given by</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" title="1"><span class="kw">def</span> model(N):</a>
<a class="sourceLine" id="cb12-2" title="2">    a <span class="op">=</span> sample(<span class="st">&quot;alpha&quot;</span>, dist.Normal(<span class="dv">0</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb12-3" title="3">    <span class="cf">with</span> plate(<span class="st">&quot;data&quot;</span>, N, dim<span class="op">=-</span><span class="dv">1</span>):</a>
<a class="sourceLine" id="cb12-4" title="4">        sample(<span class="st">&quot;obs&quot;</span>, dist.Normal(a, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb12-5" title="5">        </a>
<a class="sourceLine" id="cb12-6" title="6">N <span class="op">=</span> <span class="dv">6</span></a>
<a class="sourceLine" id="cb12-7" title="7">trace(model).get_trace(N)</a>
<a class="sourceLine" id="cb12-8" title="8"><span class="op">&gt;</span> OrderedDict([(<span class="st">'alpha'</span>,</a>
<a class="sourceLine" id="cb12-9" title="9"><span class="op">&gt;</span>               {<span class="st">'type'</span>: <span class="st">'sample'</span>,</a>
<a class="sourceLine" id="cb12-10" title="10"><span class="op">&gt;</span>                <span class="st">'name'</span>: <span class="st">'alpha'</span>,</a>
<a class="sourceLine" id="cb12-11" title="11"><span class="op">&gt;</span>                <span class="st">'fn'</span>: Normal(loc: <span class="fl">0.0</span>, scale: <span class="fl">1.0</span>),</a>
<a class="sourceLine" id="cb12-12" title="12"><span class="op">&gt;</span>                <span class="st">'args'</span>: (),</a>
<a class="sourceLine" id="cb12-13" title="13"><span class="op">&gt;</span>                <span class="st">'kwargs'</span>: {},</a>
<a class="sourceLine" id="cb12-14" title="14"><span class="op">&gt;</span>                <span class="st">'value'</span>: tensor(<span class="fl">0.4854</span>)}),</a>
<a class="sourceLine" id="cb12-15" title="15"><span class="op">&gt;</span>              (<span class="st">'obs'</span>,</a>
<a class="sourceLine" id="cb12-16" title="16"><span class="op">&gt;</span>               {<span class="st">'type'</span>: <span class="st">'sample'</span>,</a>
<a class="sourceLine" id="cb12-17" title="17"><span class="op">&gt;</span>                <span class="st">'name'</span>: <span class="st">'obs'</span>,</a>
<a class="sourceLine" id="cb12-18" title="18"><span class="op">&gt;</span>                <span class="st">'fn'</span>: Normal(loc: torch.Size([<span class="dv">6</span>]), scale: torch.Size([<span class="dv">6</span>])),</a>
<a class="sourceLine" id="cb12-19" title="19"><span class="op">&gt;</span>                <span class="st">'args'</span>: (),</a>
<a class="sourceLine" id="cb12-20" title="20"><span class="op">&gt;</span>                <span class="st">'kwargs'</span>: {},</a>
<a class="sourceLine" id="cb12-21" title="21"><span class="op">&gt;</span>                <span class="st">'value'</span>: tensor([ <span class="fl">0.0110</span>,  <span class="fl">2.6059</span>, <span class="fl">-0.6557</span>,  <span class="fl">1.2493</span>,  <span class="fl">1.0139</span>,  <span class="fl">2.2700</span>])})])</a></code></pre></div>
<p>We see that without writing a loop, we sample <code>N</code> observations from the above distribution via pure broadcasting semantics.</p>
<h3 id="inference">inference</h3>
<p>Now that we have a hang of the basic objects in <code>pyro</code>, we now want to formulate the entire reason for probabilistic programming– automatic Bayesian inference. What is <strong>inference</strong>? It is the effective manipulation of our generative models to produce estimates over latent variables in our model.</p>
<p>The basic ingredients we need for inference in <code>pyro</code> are conditioning and good inference algorithms; in our case, it is given by <em>stochastic variational inference</em>. We start with <code>condition</code>.</p>
<p><code>condition</code> is an effect handler that effectively allows us to hold certain sampling sites at fixed values. Implementing this is easy– we postprocess any message that is emitted by a <code>pyro.sample</code> site by swapping out whatever was sampled by our fixed value.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" title="1"><span class="kw">class</span> condition(Messenger):</a>
<a class="sourceLine" id="cb13-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, fn, data<span class="op">=</span><span class="va">None</span>):</a>
<a class="sourceLine" id="cb13-3" title="3">        <span class="bu">super</span>(condition, <span class="va">self</span>).<span class="fu">__init__</span>(fn)</a>
<a class="sourceLine" id="cb13-4" title="4">        <span class="va">self</span>.data <span class="op">=</span> data</a>
<a class="sourceLine" id="cb13-5" title="5">        </a>
<a class="sourceLine" id="cb13-6" title="6">    <span class="kw">def</span> process_message(<span class="va">self</span>, msg):</a>
<a class="sourceLine" id="cb13-7" title="7">        <span class="cf">if</span> <span class="va">self</span>.data <span class="kw">is</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb13-8" title="8">            <span class="cf">return</span> msg</a>
<a class="sourceLine" id="cb13-9" title="9">        </a>
<a class="sourceLine" id="cb13-10" title="10">        <span class="co"># if sample site name is in data, replace sampled value by</span></a>
<a class="sourceLine" id="cb13-11" title="11">        <span class="co"># fixed conditional value</span></a>
<a class="sourceLine" id="cb13-12" title="12">        <span class="cf">if</span> msg[<span class="st">&quot;name&quot;</span>] <span class="kw">in</span> <span class="va">self</span>.data:</a>
<a class="sourceLine" id="cb13-13" title="13">            <span class="cf">if</span> <span class="bu">isinstance</span>(<span class="va">self</span>.data[msg[<span class="st">&quot;name&quot;</span>]], torch.Tensor):</a>
<a class="sourceLine" id="cb13-14" title="14">                msg[<span class="st">&quot;value&quot;</span>] <span class="op">=</span> <span class="va">self</span>.data[msg[<span class="st">&quot;name&quot;</span>]]</a>
<a class="sourceLine" id="cb13-15" title="15">            <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb13-16" title="16">                msg[<span class="st">&quot;value&quot;</span>] <span class="op">=</span> torch.tensor(<span class="va">self</span>.data[msg[<span class="st">&quot;name&quot;</span>]])</a>
<a class="sourceLine" id="cb13-17" title="17">   </a>
<a class="sourceLine" id="cb13-18" title="18">        <span class="cf">return</span> msg</a></code></pre></div>
<p>We’ll test that it works by using it to compute the log-joint probabilities of a toy model from the <code>pyro</code> documentation.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" title="1"><span class="kw">def</span> scale(guess):</a>
<a class="sourceLine" id="cb14-2" title="2">    weight <span class="op">=</span> sample(<span class="st">&quot;weight&quot;</span>, dist.Normal(guess, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb14-3" title="3">    <span class="cf">return</span> sample(<span class="st">&quot;measurement&quot;</span>, dist.Normal(weight, <span class="fl">0.75</span>))</a>
<a class="sourceLine" id="cb14-4" title="4"></a>
<a class="sourceLine" id="cb14-5" title="5"><span class="kw">def</span> make_log_joint(model):</a>
<a class="sourceLine" id="cb14-6" title="6">    <span class="kw">def</span> _log_joint(cond_data, <span class="op">*</span>args, <span class="op">**</span>kwargs):</a>
<a class="sourceLine" id="cb14-7" title="7">        conditioned_model <span class="op">=</span> condition(model, data<span class="op">=</span>cond_data)</a>
<a class="sourceLine" id="cb14-8" title="8">        sample_trace <span class="op">=</span> trace(conditioned_model).get_trace(<span class="op">*</span>args, <span class="op">**</span>kwargs)</a>
<a class="sourceLine" id="cb14-9" title="9">        <span class="cf">return</span> log_prob_sum(sample_trace)</a>
<a class="sourceLine" id="cb14-10" title="10">    <span class="cf">return</span> _log_joint</a>
<a class="sourceLine" id="cb14-11" title="11"></a>
<a class="sourceLine" id="cb14-12" title="12">scale_log_joint <span class="op">=</span> make_log_joint(scale)</a>
<a class="sourceLine" id="cb14-13" title="13">scale_log_joint({<span class="st">&quot;measurement&quot;</span>: <span class="fl">9.5</span>, <span class="st">&quot;weight&quot;</span>: <span class="fl">8.23</span>}, <span class="fl">8.5</span>)</a>
<a class="sourceLine" id="cb14-14" title="14"><span class="op">&gt;</span> tensor(<span class="op">-</span><span class="fl">3.0203</span>)</a></code></pre></div>
<p>Often what we want to do is estimate the latent variables in a generative model that best estimates the observational data the model is conditioned on. However, the <code>sample</code> statements in our models are immobile and our inference algorithms won’t operate on them. Instead, we need to represent latent variables by <code>param</code>eters.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" title="1"><span class="kw">def</span> param(name, init_value<span class="op">=</span><span class="va">None</span>, constraint<span class="op">=</span>torch.distributions.constraints.real, event_dim<span class="op">=</span><span class="va">None</span>):</a>
<a class="sourceLine" id="cb15-2" title="2">    <span class="cf">if</span> event_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</a>
<a class="sourceLine" id="cb15-3" title="3">        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="st">&quot;plate does not support the event_dim arg&quot;</span>)</a>
<a class="sourceLine" id="cb15-4" title="4">    </a>
<a class="sourceLine" id="cb15-5" title="5">    <span class="co"># a parameter needs to be optimized in an unconstrained space, while the underlying</span></a>
<a class="sourceLine" id="cb15-6" title="6">    <span class="co"># parameter itself could be very constrained. we need a function that brokers the</span></a>
<a class="sourceLine" id="cb15-7" title="7">    <span class="co"># passage between the two spaces</span></a>
<a class="sourceLine" id="cb15-8" title="8">    <span class="kw">def</span> fn(init_value, constraint):</a>
<a class="sourceLine" id="cb15-9" title="9">        <span class="co"># remember, PARAM_STORE maps name -&gt; (unconstrained_value, constraint)</span></a>
<a class="sourceLine" id="cb15-10" title="10">        <span class="cf">if</span> name <span class="kw">in</span> PARAM_STORE:</a>
<a class="sourceLine" id="cb15-11" title="11">            unconstrained_value, constraint <span class="op">=</span> PARAM_STORE[name]</a>
<a class="sourceLine" id="cb15-12" title="12">        <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb15-13" title="13">            <span class="co"># initialize with a constrained value</span></a>
<a class="sourceLine" id="cb15-14" title="14">            <span class="cf">assert</span> init_value <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></a>
<a class="sourceLine" id="cb15-15" title="15">            <span class="cf">with</span> torch.no_grad():</a>
<a class="sourceLine" id="cb15-16" title="16">                constrained_value <span class="op">=</span> init_value.detach()</a>
<a class="sourceLine" id="cb15-17" title="17">                unconstrained_value <span class="op">=</span> torch.distributions.transform_to(constraint).inv(constrained_value)</a>
<a class="sourceLine" id="cb15-18" title="18">            <span class="co"># as the unconstrained parameter needs to be optimized, make sure</span></a>
<a class="sourceLine" id="cb15-19" title="19">            <span class="co"># gradients can propagate through node</span></a>
<a class="sourceLine" id="cb15-20" title="20">            unconstrained_value.requires_grad_()</a>
<a class="sourceLine" id="cb15-21" title="21">            PARAM_STORE[name] <span class="op">=</span> (unconstrained_value, constraint)</a>
<a class="sourceLine" id="cb15-22" title="22"></a>
<a class="sourceLine" id="cb15-23" title="23">        <span class="co"># transform from unconstrained space to constrained space.</span></a>
<a class="sourceLine" id="cb15-24" title="24">        constrained_value <span class="op">=</span> torch.distributions.transform_to(constraint)(unconstrained_value)</a>
<a class="sourceLine" id="cb15-25" title="25">        constrained_value.unconstrained <span class="op">=</span> weakref.ref(unconstrained_value)</a>
<a class="sourceLine" id="cb15-26" title="26">        <span class="cf">return</span> constrained_value</a>
<a class="sourceLine" id="cb15-27" title="27">    </a>
<a class="sourceLine" id="cb15-28" title="28">    <span class="co"># if there are no active Messengers, we just draw a sample and return it as expected</span></a>
<a class="sourceLine" id="cb15-29" title="29">    <span class="cf">if</span> <span class="kw">not</span> PYRO_STACK:</a>
<a class="sourceLine" id="cb15-30" title="30">        <span class="cf">return</span> fn(init_value, constraint)</a>
<a class="sourceLine" id="cb15-31" title="31">    </a>
<a class="sourceLine" id="cb15-32" title="32">    <span class="co"># else, an effect (msg) is initialized</span></a>
<a class="sourceLine" id="cb15-33" title="33">    effect_msg <span class="op">=</span> {</a>
<a class="sourceLine" id="cb15-34" title="34">        <span class="st">&quot;type&quot;</span>: <span class="st">&quot;param&quot;</span>,</a>
<a class="sourceLine" id="cb15-35" title="35">        <span class="st">&quot;name&quot;</span>: name,     <span class="co"># parameter sites are given unique names</span></a>
<a class="sourceLine" id="cb15-36" title="36">        <span class="st">&quot;fn&quot;</span>: fn,</a>
<a class="sourceLine" id="cb15-37" title="37">        <span class="st">&quot;args&quot;</span>: (init_value, constraint),</a>
<a class="sourceLine" id="cb15-38" title="38">        <span class="st">&quot;value&quot;</span>: <span class="va">None</span>}</a>
<a class="sourceLine" id="cb15-39" title="39">    </a>
<a class="sourceLine" id="cb15-40" title="40">    <span class="co"># pass it through the effect handlers</span></a>
<a class="sourceLine" id="cb15-41" title="41">    processed_effect_msg <span class="op">=</span> apply_stack(effect_msg)</a>
<a class="sourceLine" id="cb15-42" title="42">    <span class="cf">return</span> processed_effect_msg[<span class="st">&quot;value&quot;</span>]</a></code></pre></div>
<p>The complexity of <code>param</code> is only from the fact that 1) we need to be able to impose optimizable constraints on our parameters, and 2) we need to be able to apply autodifferentiation to optimize said parameters.</p>
<p>Now all we need is an inference algorithm. The hallmark of <code>pyro</code> is its usage of <strong>stochastic variational inference</strong>, which can be easily describe as trying to approximate the posterior distribution <span class="math inline">\(p(\theta|\text{data})\)</span> of our model via a parameterized family of <strong>guides</strong> <span class="math inline">\(q(\theta; \text{params})\)</span> where <span class="math inline">\(\text{params}\)</span> are optimizable params. We do this by finding the parameters that make our guides as close as possible to the posterior, where distance here is given by the Kullback-Leibner divergence</p>
<p><span class="math display">\[ \text{D}_\text{KL}(q(\theta;\text{params})|| p(\theta|\text{data})) \]</span></p>
<p>For example, in our <code>scale</code> example above, the posterior <span class="math inline">\(p(\text{weight}|\text{meas}=9.5, \text{guess}=8.5)\)</span> is some univariate distribution. We will try and approximate it with the family of normal distributions <span class="math inline">\(q(\text{weight}|a, b)\)</span>:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" title="1"><span class="kw">def</span> scale_guide(guess):</a>
<a class="sourceLine" id="cb16-2" title="2">    a <span class="op">=</span> param(<span class="st">&quot;a&quot;</span>, torch.tensor(guess))</a>
<a class="sourceLine" id="cb16-3" title="3">    b <span class="op">=</span> param(<span class="st">&quot;b&quot;</span>, torch.tensor(<span class="fl">1.0</span>), constraint<span class="op">=</span>torch.distributions.constraints.positive)</a>
<a class="sourceLine" id="cb16-4" title="4">    <span class="cf">return</span> sample(<span class="st">&quot;weight&quot;</span>, dist.Normal(a, b))</a></code></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb17-1" title="1">trace(scale_guide).get_trace(<span class="fl">3.0</span>)</a>
<a class="sourceLine" id="cb17-2" title="2"><span class="op">&gt;</span> OrderedDict([(<span class="st">'a'</span>,</a>
<a class="sourceLine" id="cb17-3" title="3"><span class="op">&gt;</span>               {<span class="st">'type'</span>: <span class="st">'param'</span>,</a>
<a class="sourceLine" id="cb17-4" title="4"><span class="op">&gt;</span>                <span class="st">'name'</span>: <span class="st">'a'</span>,</a>
<a class="sourceLine" id="cb17-5" title="5"><span class="op">&gt;</span>                <span class="st">'fn'</span>: <span class="op">&lt;</span>function __main__.param.<span class="op">&lt;</span><span class="bu">locals</span><span class="op">&gt;</span>.fn(init_value, constraint)<span class="op">&gt;</span>,</a>
<a class="sourceLine" id="cb17-6" title="6"><span class="op">&gt;</span>                <span class="st">'args'</span>: (tensor(<span class="fl">3.</span>), Real()),</a>
<a class="sourceLine" id="cb17-7" title="7"><span class="op">&gt;</span>                <span class="st">'value'</span>: tensor(<span class="fl">3.</span>, requires_grad<span class="op">=</span><span class="va">True</span>)}),</a>
<a class="sourceLine" id="cb17-8" title="8"><span class="op">&gt;</span>              (<span class="st">'b'</span>,</a>
<a class="sourceLine" id="cb17-9" title="9"><span class="op">&gt;</span>               {<span class="st">'type'</span>: <span class="st">'param'</span>,</a>
<a class="sourceLine" id="cb17-10" title="10"><span class="op">&gt;</span>                <span class="st">'name'</span>: <span class="st">'b'</span>,</a>
<a class="sourceLine" id="cb17-11" title="11"><span class="op">&gt;</span>                <span class="st">'fn'</span>: <span class="op">&lt;</span>function __main__.param.<span class="op">&lt;</span><span class="bu">locals</span><span class="op">&gt;</span>.fn(init_value, constraint)<span class="op">&gt;</span>,</a>
<a class="sourceLine" id="cb17-12" title="12"><span class="op">&gt;</span>                <span class="st">'args'</span>: (tensor(<span class="fl">1.</span>), GreaterThan(lower_bound<span class="op">=</span><span class="fl">0.0</span>)),</a>
<a class="sourceLine" id="cb17-13" title="13"><span class="op">&gt;</span>                <span class="st">'value'</span>: tensor(<span class="fl">1.</span>, grad_fn<span class="op">=&lt;</span>AddBackward0<span class="op">&gt;</span>)}),</a>
<a class="sourceLine" id="cb17-14" title="14"><span class="op">&gt;</span>              (<span class="st">'weight'</span>,</a>
<a class="sourceLine" id="cb17-15" title="15"><span class="op">&gt;</span>               {<span class="st">'type'</span>: <span class="st">'sample'</span>,</a>
<a class="sourceLine" id="cb17-16" title="16"><span class="op">&gt;</span>                <span class="st">'name'</span>: <span class="st">'weight'</span>,</a>
<a class="sourceLine" id="cb17-17" title="17"><span class="op">&gt;</span>                <span class="st">'fn'</span>: Normal(loc: <span class="fl">3.0</span>, scale: <span class="fl">1.0</span>),</a>
<a class="sourceLine" id="cb17-18" title="18"><span class="op">&gt;</span>                <span class="st">'args'</span>: (),</a>
<a class="sourceLine" id="cb17-19" title="19"><span class="op">&gt;</span>                <span class="st">'kwargs'</span>: {},</a>
<a class="sourceLine" id="cb17-20" title="20"><span class="op">&gt;</span>                <span class="st">'value'</span>: tensor(<span class="fl">1.0948</span>, grad_fn<span class="op">=&lt;</span>AddBackward0<span class="op">&gt;</span>)})])</a></code></pre></div>
<p>This is our guide <span class="math inline">\(q(\theta|\text{params})\)</span>. Our goal is to find the parameter that minimizes the KL-divergence (also known as the <span class="math inline">\(\text{ELBO}\)</span> (<strong>evidence lower bound</strong>)):</p>
<p><span class="math display">\[ \text{D}_\text{KL}(q(\theta;\text{params})|| p(\theta|\text{data})) =
    \mathbf{E}_{\theta\sim q(\theta;\text{params})}[\log{q(\theta;\text{params})} - \log{p(\theta|\text{data})}]
\]</span></p>
<p>This will be computed using a 1-sample monte-carlo estimator of the <span class="math inline">\(\text{ELBO}\)</span>, which we call <code>Trace_ELBO</code>. There is one technical point to be made here– once we sample a trace from our guide <span class="math inline">\(q(\theta;\text{params})\)</span>, we must use <strong>the exact same trace</strong> to compute the log-probabilities for both <span class="math inline">\(q\)</span> and <span class="math inline">\(p\)</span>. In essence, we need to “replay” the trace for the model underlying <span class="math inline">\(p(\theta|\text{data})\)</span> and use those values to compute the log-probability. This can be seen by the expectation above– a monte-carlo estimate of this expectation samples <span class="math inline">\(theta\)</span> from <span class="math inline">\(q(\theta;\text{params})\)</span> and then uses these same <span class="math inline">\(\theta\)</span> in the log-probs.</p>
<p>This <code>replay</code> is itself an effect handler– intuitively during a replay, any sample effect msg emitted by the model is intercepted and has its value replaced by the trace its replaying.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb18-1" title="1"><span class="kw">class</span> replay(Messenger):</a>
<a class="sourceLine" id="cb18-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, fn, guide_trace):</a>
<a class="sourceLine" id="cb18-3" title="3">        <span class="va">self</span>.guide_trace <span class="op">=</span> guide_trace</a>
<a class="sourceLine" id="cb18-4" title="4">        <span class="bu">super</span>(replay, <span class="va">self</span>).<span class="fu">__init__</span>(fn)</a>
<a class="sourceLine" id="cb18-5" title="5">        </a>
<a class="sourceLine" id="cb18-6" title="6">    <span class="kw">def</span> process_message(<span class="va">self</span>, msg):</a>
<a class="sourceLine" id="cb18-7" title="7">        <span class="cf">if</span> msg[<span class="st">&quot;name&quot;</span>] <span class="kw">in</span> <span class="va">self</span>.guide_trace:</a>
<a class="sourceLine" id="cb18-8" title="8">            <span class="co"># replaces model sampled value with value in guide trace</span></a>
<a class="sourceLine" id="cb18-9" title="9">            msg[<span class="st">&quot;value&quot;</span>] <span class="op">=</span> <span class="va">self</span>.guide_trace[msg[<span class="st">&quot;name&quot;</span>]][<span class="st">&quot;value&quot;</span>]</a></code></pre></div>
<p>Now we use this to compute the <span class="math inline">\(\text{ELBO}\)</span> via Monte Carlo:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb19-1" title="1"><span class="kw">def</span> elbo(model, guide, <span class="op">*</span>args, <span class="op">**</span>kwargs):</a>
<a class="sourceLine" id="cb19-2" title="2">    <span class="co"># sample from the guide to get a trace of params/samples</span></a>
<a class="sourceLine" id="cb19-3" title="3">    guide_trace <span class="op">=</span> trace(guide).get_trace(<span class="op">*</span>args, <span class="op">**</span>kwargs)</a>
<a class="sourceLine" id="cb19-4" title="4">    <span class="co"># replay the guide trace on the model to get a trace of model</span></a>
<a class="sourceLine" id="cb19-5" title="5">    model_trace <span class="op">=</span> trace(replay(model, guide_trace)).get_trace(<span class="op">*</span>args, <span class="op">**</span>kwargs)</a>
<a class="sourceLine" id="cb19-6" title="6">    </a>
<a class="sourceLine" id="cb19-7" title="7">    elbo <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb19-8" title="8">    <span class="co"># go through trace, adding up log probabilities</span></a>
<a class="sourceLine" id="cb19-9" title="9">    <span class="cf">for</span> site <span class="kw">in</span> model_trace.values():</a>
<a class="sourceLine" id="cb19-10" title="10">        <span class="cf">if</span> site[<span class="st">&quot;type&quot;</span>] <span class="op">==</span> <span class="st">&quot;sample&quot;</span>:</a>
<a class="sourceLine" id="cb19-11" title="11">            elbo <span class="op">=</span> elbo <span class="op">-</span> site[<span class="st">&quot;fn&quot;</span>].log_prob(site[<span class="st">&quot;value&quot;</span>]).<span class="bu">sum</span>()</a>
<a class="sourceLine" id="cb19-12" title="12">    <span class="cf">for</span> site <span class="kw">in</span> guide_trace.values():</a>
<a class="sourceLine" id="cb19-13" title="13">        <span class="cf">if</span> site[<span class="st">&quot;type&quot;</span>] <span class="op">==</span> <span class="st">&quot;sample&quot;</span>:</a>
<a class="sourceLine" id="cb19-14" title="14">            elbo <span class="op">=</span> elbo <span class="op">+</span> site[<span class="st">&quot;fn&quot;</span>].log_prob(site[<span class="st">&quot;value&quot;</span>]).<span class="bu">sum</span>()</a>
<a class="sourceLine" id="cb19-15" title="15">            </a>
<a class="sourceLine" id="cb19-16" title="16">    <span class="cf">return</span> elbo</a>
<a class="sourceLine" id="cb19-17" title="17"></a>
<a class="sourceLine" id="cb19-18" title="18"><span class="kw">def</span> Trace_ELBO():</a>
<a class="sourceLine" id="cb19-19" title="19">    <span class="cf">return</span> elbo</a></code></pre></div>
<p>Finally, we can wrap up the training loop for our inference algorithm into a stochastic variational inference class! Since we are interested in optimizing the <code>param</code> tensors in our algorithm, we need a way to isolate the <code>param</code> messages from <code>sample</code> statements in our model. We do this by a separate trace of our model that “blocks” out the <code>sample</code> statements. as you guessed it, it is another effect handler– <code>block</code>:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb20-1" title="1"><span class="kw">class</span> block(Messenger):</a>
<a class="sourceLine" id="cb20-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, fn<span class="op">=</span><span class="va">None</span>, hide_fn<span class="op">=</span><span class="kw">lambda</span> msg: <span class="va">True</span>):</a>
<a class="sourceLine" id="cb20-3" title="3">        <span class="va">self</span>.hide_fn <span class="op">=</span> hide_fn</a>
<a class="sourceLine" id="cb20-4" title="4">        <span class="bu">super</span>().<span class="fu">__init__</span>(fn)</a>
<a class="sourceLine" id="cb20-5" title="5"></a>
<a class="sourceLine" id="cb20-6" title="6">    <span class="kw">def</span> process_message(<span class="va">self</span>, msg):</a>
<a class="sourceLine" id="cb20-7" title="7">        <span class="co"># the hide_fn is a bool-valued function that checks if the message</span></a>
<a class="sourceLine" id="cb20-8" title="8">        <span class="co"># is not allowed to propagate further</span></a>
<a class="sourceLine" id="cb20-9" title="9">        <span class="cf">if</span> <span class="va">self</span>.hide_fn(msg):</a>
<a class="sourceLine" id="cb20-10" title="10">            msg[<span class="st">&quot;stop&quot;</span>] <span class="op">=</span> <span class="va">True</span></a></code></pre></div>
<p>All of our effect handlers cumulate in our <code>SVI</code> class:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb21-1" title="1"><span class="kw">class</span> SVI:</a>
<a class="sourceLine" id="cb21-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, guide, optim, loss):</a>
<a class="sourceLine" id="cb21-3" title="3">        <span class="va">self</span>.model <span class="op">=</span> model</a>
<a class="sourceLine" id="cb21-4" title="4">        <span class="va">self</span>.guide <span class="op">=</span> guide</a>
<a class="sourceLine" id="cb21-5" title="5">        <span class="va">self</span>.optim <span class="op">=</span> optim</a>
<a class="sourceLine" id="cb21-6" title="6">        <span class="va">self</span>.loss <span class="op">=</span> loss</a>
<a class="sourceLine" id="cb21-7" title="7">        </a>
<a class="sourceLine" id="cb21-8" title="8">    <span class="kw">def</span> step(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</a>
<a class="sourceLine" id="cb21-9" title="9">        <span class="co">&quot;&quot;&quot;performs a single step of optimization&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb21-10" title="10">        <span class="co"># trace only the param messages emitted by our guide</span></a>
<a class="sourceLine" id="cb21-11" title="11">        <span class="cf">with</span> trace() <span class="im">as</span> param_capture:</a>
<a class="sourceLine" id="cb21-12" title="12">            <span class="cf">with</span> block(hide_fn<span class="op">=</span><span class="kw">lambda</span> msg: msg[<span class="st">&quot;type&quot;</span>] <span class="op">==</span> <span class="st">&quot;sample&quot;</span>):</a>
<a class="sourceLine" id="cb21-13" title="13">                loss <span class="op">=</span> <span class="va">self</span>.loss(<span class="va">self</span>.model, <span class="va">self</span>.guide, <span class="op">*</span>args, <span class="op">**</span>kwargs)</a>
<a class="sourceLine" id="cb21-14" title="14">        </a>
<a class="sourceLine" id="cb21-15" title="15">        <span class="co"># take gradients and propagate</span></a>
<a class="sourceLine" id="cb21-16" title="16">        loss.backward()</a>
<a class="sourceLine" id="cb21-17" title="17">        </a>
<a class="sourceLine" id="cb21-18" title="18">        <span class="co"># extract parameters from the trace</span></a>
<a class="sourceLine" id="cb21-19" title="19">        params <span class="op">=</span> [param_site[<span class="st">&quot;value&quot;</span>].unconstrained() </a>
<a class="sourceLine" id="cb21-20" title="20">                  <span class="cf">for</span> param_site <span class="kw">in</span> param_capture.values()]</a>
<a class="sourceLine" id="cb21-21" title="21">        <span class="va">self</span>.optim(params)</a>
<a class="sourceLine" id="cb21-22" title="22">        </a>
<a class="sourceLine" id="cb21-23" title="23">        <span class="co"># manually perform zero_grad</span></a>
<a class="sourceLine" id="cb21-24" title="24">        <span class="cf">for</span> param <span class="kw">in</span> params:</a>
<a class="sourceLine" id="cb21-25" title="25">            param.grad <span class="op">=</span> torch.zeros_like(param)</a>
<a class="sourceLine" id="cb21-26" title="26">        </a>
<a class="sourceLine" id="cb21-27" title="27">        <span class="cf">return</span> loss.item()</a></code></pre></div>
<p>Here, <code>torch</code> optimizers should be wrapped as parameters can be dynamically created during the training process.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb22-1" title="1"><span class="kw">class</span> Adam:</a>
<a class="sourceLine" id="cb22-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, optim_args):</a>
<a class="sourceLine" id="cb22-3" title="3">        <span class="va">self</span>.optim_args <span class="op">=</span> optim_args</a>
<a class="sourceLine" id="cb22-4" title="4">        <span class="co"># each parameter will get its own optimizer, which we keep track</span></a>
<a class="sourceLine" id="cb22-5" title="5">        <span class="co"># of using this dictionary keyed on parameters.</span></a>
<a class="sourceLine" id="cb22-6" title="6">        <span class="va">self</span>.optim_objs <span class="op">=</span> {}</a>
<a class="sourceLine" id="cb22-7" title="7"></a>
<a class="sourceLine" id="cb22-8" title="8">    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, params):</a>
<a class="sourceLine" id="cb22-9" title="9">        <span class="cf">for</span> param <span class="kw">in</span> params:</a>
<a class="sourceLine" id="cb22-10" title="10">            <span class="co"># If we've seen this parameter before, use the previously</span></a>
<a class="sourceLine" id="cb22-11" title="11">            <span class="co"># constructed optimizer.</span></a>
<a class="sourceLine" id="cb22-12" title="12">            <span class="cf">if</span> param <span class="kw">in</span> <span class="va">self</span>.optim_objs:</a>
<a class="sourceLine" id="cb22-13" title="13">                optim <span class="op">=</span> <span class="va">self</span>.optim_objs[param]</a>
<a class="sourceLine" id="cb22-14" title="14">            <span class="co"># If we've never seen this parameter before, construct</span></a>
<a class="sourceLine" id="cb22-15" title="15">            <span class="co"># an Adam optimizer and keep track of it.</span></a>
<a class="sourceLine" id="cb22-16" title="16">            <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb22-17" title="17">                optim <span class="op">=</span> torch.optim.Adam([param], <span class="op">**</span><span class="va">self</span>.optim_args)</a>
<a class="sourceLine" id="cb22-18" title="18">                <span class="va">self</span>.optim_objs[param] <span class="op">=</span> optim</a>
<a class="sourceLine" id="cb22-19" title="19">            <span class="co"># Take a gradient step for the parameter param.</span></a>
<a class="sourceLine" id="cb22-20" title="20">            optim.step()</a></code></pre></div>
<p>Let’s test this out on our <code>scale</code> model!</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb23-1" title="1"><span class="im">from</span> tqdm <span class="im">import</span> tqdm</a>
<a class="sourceLine" id="cb23-2" title="2"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb23-3" title="3"></a>
<a class="sourceLine" id="cb23-4" title="4">n_epochs <span class="op">=</span> <span class="dv">2500</span></a>
<a class="sourceLine" id="cb23-5" title="5">optim <span class="op">=</span> Adam({<span class="st">&quot;lr&quot;</span>: <span class="fl">0.007</span>, <span class="st">&quot;betas&quot;</span>: (<span class="fl">0.90</span>, <span class="fl">0.999</span>)})</a>
<a class="sourceLine" id="cb23-6" title="6"></a>
<a class="sourceLine" id="cb23-7" title="7"><span class="co"># condition scale model on conditioned values</span></a>
<a class="sourceLine" id="cb23-8" title="8">conditioned_values <span class="op">=</span> {<span class="st">&quot;measurement&quot;</span>: <span class="fl">9.5</span>}</a>
<a class="sourceLine" id="cb23-9" title="9">conditioned_model <span class="op">=</span> condition(scale, conditioned_values)</a>
<a class="sourceLine" id="cb23-10" title="10"></a>
<a class="sourceLine" id="cb23-11" title="11">svi <span class="op">=</span> SVI(model<span class="op">=</span>conditioned_model,</a>
<a class="sourceLine" id="cb23-12" title="12">          guide<span class="op">=</span>scale_guide,</a>
<a class="sourceLine" id="cb23-13" title="13">          optim<span class="op">=</span>optim,</a>
<a class="sourceLine" id="cb23-14" title="14">          loss<span class="op">=</span>Trace_ELBO())</a>
<a class="sourceLine" id="cb23-15" title="15"></a>
<a class="sourceLine" id="cb23-16" title="16">guess <span class="op">=</span> <span class="fl">8.5</span></a>
<a class="sourceLine" id="cb23-17" title="17"></a>
<a class="sourceLine" id="cb23-18" title="18"><span class="co"># run variational inference</span></a>
<a class="sourceLine" id="cb23-19" title="19">losses <span class="op">=</span> []</a>
<a class="sourceLine" id="cb23-20" title="20"><span class="cf">for</span> _ <span class="kw">in</span> tqdm(<span class="bu">range</span>(n_epochs)):</a>
<a class="sourceLine" id="cb23-21" title="21">    loss <span class="op">=</span> svi.step(guess)</a>
<a class="sourceLine" id="cb23-22" title="22">    losses.append(loss)</a></code></pre></div>
<p>Plotting the loss gives</p>
<p align="center">
<img width="560" height="300" src="../images/elbo_loss.png">
</p>
<p>Now we can extract the posterior parameter values inferred by our variational inference procedure.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb24-1" title="1"><span class="co"># extract posterior parameter values</span></a>
<a class="sourceLine" id="cb24-2" title="2"><span class="bu">print</span>(<span class="st">&quot;a = </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(PARAM_STORE[<span class="st">&quot;a&quot;</span>][<span class="dv">0</span>].item()))</a>
<a class="sourceLine" id="cb24-3" title="3"><span class="op">&gt;</span> a <span class="op">=</span> <span class="fl">9.12030029296875</span></a>
<a class="sourceLine" id="cb24-4" title="4"></a>
<a class="sourceLine" id="cb24-5" title="5"><span class="bu">print</span>(<span class="st">&quot;b = </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(torch.<span class="bu">abs</span>(PARAM_STORE[<span class="st">&quot;b&quot;</span>][<span class="dv">0</span>]).item()))</a>
<a class="sourceLine" id="cb24-6" title="6"><span class="op">&gt;</span> b <span class="op">=</span> <span class="fl">0.49517199397087097</span></a></code></pre></div>
<p>This is close to what can be inferred analytically for this problem!</p>
<p>As we see, flexible algebraic effect systems allow us to build complicated inference algorithms without thinking too much about the emitted effects. For example, with multi-shot delimited effects we can build inference algorithms that allow us to fully enumerate over discrete latent variables. This is the idea behind the construction of <code>pyro.poutine.queue</code>.</p>
<h3 id="mcmc">mcmc</h3>
<p><strong>Note:</strong> Write this section at some point when I have time.</p>
    </section>
</article>

        </main>

        <footer>
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </footer>
    </body>
</html>
