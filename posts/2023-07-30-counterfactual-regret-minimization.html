<!doctype html>
<html lang="en">
    <head>
        <link rel="icon" type="image/x-icon" href="../favicon.ico" />
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Counterfactual regret minimization - Calvin Woo's blog</title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../css/syntax.css" />
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG" crossorigin="anonymous">
    </head>
    <body>
        <header>
            <div class="logo">
                <a href="../">Calvin's Notebook</a>
            </div>
            <nav>
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../archive.html">Archive</a>
            </nav>
        </header>

        <main role="main">
            <h1>Counterfactual regret minimization</h1>
            <article>
    <section class="header">
        Posted on July 30, 2023
        
            by Calvin
        
    </section>
    <section>
        <p>For the past few days I’ve been wanting to learn about modern game-playing AIs, and what are the main techniques out there powering them. I remember years ago when I started reading computer science books about heuristic search techniques and programming a Tic-Tac-Toe AI using various tree search algorithms. I was fairly interested in reinforcement learning for awhile, read about MCTS and DeepMind’s successes with AlphaGo, but then I got busy with other stuff and ended up not reading much on it afterwards.</p>
<p>Recently I’ve started to get back into reading about reinforcement learning for <em>imperfect information games</em> (games in which each player has private information about the state of the game that they do not share with each other). It turns out that many of these techniques require a fair amount of background in game theory and their solution concepts, so I figured I’d write some notes on the topic.</p>
<p>In this post, which should be the first of at least 2 more, we will give a brief overview of the definitions and concepts from classical game theory that we need, restricting ourselves to two-player zero-sum games. Then we’ll introduce the optimization idea of regret minimization, and give an overview of counterfactual regret minimization, an efficient implementation of regret minimization for imperfect information games.</p>
<h3 id="game-theory">game theory</h3>
<p>In this section we will give an brief introduction to some game-theoretic concepts, rushing to introduce the central concept of a Nash equilibrium of a normal-form game.</p>
<p>First, we should define what we mean by a <strong>normal form game</strong>. We will be considering games where there are multiple <em>agents</em> (say, <span class="math inline">\(n\)</span> of them), and where each agent can take a certain action <span class="math inline">\(a_i\)</span> from it’s <em>action space</em> <span class="math inline">\(A_i\)</span>. In these games, all agents perform their action simultaneously, with the actions of the other agents not influencing immediately each other. In this way, the game’s outcome is determined by the single <em>action profile</em> <span class="math display">\[ a = (a_1, ..., a_n) \in A = A_1 \times ... \times A_n \]</span> What would cause an agent to take a particular action over others? In this setting we assume each agent is equipped with a <em>utility function</em> <span class="math inline">\(u_i: A\rightarrow \mathbb{R}\)</span> that effectively gives the payoff that each action profile gives to the <span class="math inline">\(i\)</span>th agent. It is then a fair assumption that the goal of an agent is to maximize their payoff.</p>
<p>It can be convenient (especially in low-dimensions) to represent a normal-form game as a <span class="math inline">\(n\)</span>-dimensional matrix of payoffs. For example, a canonical example is the prisoner’s dilemma problem</p>
<pre><code>           SILENT     CONFESS
        -----------------------
SILENT  | (-1, -1) | (-5, 0)  |
CONFESS | (0, -5)  | (-3, -3) |
        -----------------------</code></pre>
<p>Here, the rows denote the action of the first player, while the columns are the action of the second player. For example, if player 1 confesses and player 2 stays silent, the entry <span class="math inline">\((0, -5)\)</span> means player 1 gets a payoff of 0 and player 2 gets a payoff of -5.</p>
<p>We mention that we are most interested at the moment in <strong>zero-sum games</strong>, which are a particular class of two player normal-form games in which the sum of the payoffs of the agents is always 0, regardless of the action profile. Zero-sum games encapsulates games in which “one player’s loss is another’s gain”.</p>
<h3 id="pareto-optimality">pareto optimality</h3>
<p>How an agent chooses an action from its action space in a game is called it’s <strong>strategy</strong>. It could select a single action and play it, which is called a <strong>pure strategy</strong>. However, it is convenient if an agent can choose among many different actions, where the action to play is sampled from a probability distribution. The choice of a distribution over actions is called a <strong>mixed strategy</strong>.</p>
<p>In a normal-form game, each agent takes a strategy <span class="math inline">\(s_i\)</span>, which collectively forms a <em>strategy profile</em> <span class="math inline">\(s=(s_1,...,s_n)\)</span>. We can extend each agent’s utility function to a function on strategy profiles <span class="math inline">\(u_i(s_1,...,s_n)\)</span> by defining it as the expected utility marginalized over the joint distribution of action profiles.</p>
<p>Since the goal of every agent is to maximize their utility, this induces a strict partial ordering on the set of (mixed) strategy profiles: a profile <span class="math inline">\(s\)</span> <strong>Pareto dominates</strong> another strategy profile <span class="math inline">\(s'\)</span> if for all agents <span class="math inline">\(i\)</span>, <span class="math inline">\(u_i(s) \ge u_i(s'\)</span>), and there exists an agent <span class="math inline">\(j\)</span> in which <span class="math inline">\(u_j(s) &gt; u_j(s')\)</span>. In other words, in a Pareto dominated strategy profile some player can be made better off without making any other player worse off. The optima of this partial ordering are called <strong>Pareto optimal</strong> strategies.</p>
<p>A few observations about Pareto optimality: first, every game has at least one Pareto optimal strategy. In fact, we can always find a Pareto optimal strategy composed of pure strategies for each agent– for example, for agent <span class="math inline">\(1\)</span>, we can consider the set of pure strategy profiles with the highest payoff for them (there may be multiple action profiles with the same payoff). Then for agent <span class="math inline">\(2\)</span>, we choose the subset of that set that has the highest payoff for them, etc. The resulting pure strategy profiles are all Pareto optimal. Second, in a zero-sum game, all strategy profiles are Pareto optimal.</p>
<h3 id="nash-equilibrium">nash equilibrium</h3>
<p>For each agent <span class="math inline">\(i\)</span>, let <span class="math inline">\(s_i\)</span> denote a (mixed) strategy the agent could take, and <span class="math inline">\(s_{-i}=(s_1,...,s_{i-1}, s_{i+1},...,s_n)\)</span> a strategy profile without agent <span class="math inline">\(i\)</span> considered. That is, we are isolating agent <span class="math inline">\(i\)</span>’s strategy from a full strategy profile <span class="math inline">\(s=(s_i, s_{-i})\)</span> for the game.</p>
<p>If agent <span class="math inline">\(i\)</span> knew the strategy profile <span class="math inline">\(s_{-i}\)</span> of the other agents, it could choose the strategy <span class="math inline">\(s^*_i\)</span> that would be benefit itself. That is, for any other strategy <span class="math inline">\(s'_i\)</span>, <span class="math display">\[ u_i(s^*_i, s_{-i}) \ge u_i(s'_i, s_{-i}) \]</span> We call <span class="math inline">\(s^*_i\)</span> the <strong>best response</strong> of agent <span class="math inline">\(i\)</span> to the other’s strategy <span class="math inline">\(s_{-i}\)</span>.</p>
<p>If the other agents knew of each other’s strategies, they would all be continually trying to change their strategies in order to implement the best responses to one another. Iteratively, they may eventually reach a stable state– they may reach a strategy profile <span class="math inline">\(s=(s_1,...,s_n)\)</span> in which for all agents <span class="math inline">\(i\)</span>, <span class="math inline">\(s_i\)</span> is a best response to the remainder <span class="math inline">\(s_{-i}\)</span>. This is a <strong>Nash equilibrium</strong>. Note that equilibria may not be unique.</p>
<p>Since we’re working with zero-sum games, there are a couple of properties Nash equilibria enjoy that we will use. For one, in zero-sum games the Nash equilibria are <em>interchangable</em>: if <span class="math inline">\(s=(s_1, s_2)\)</span> is a Nash equilibrium and <span class="math inline">\(s' = (s'_1, s'_2)\)</span> is another, then <span class="math inline">\((s_1, s'_2)\)</span> and <span class="math inline">\((s'_1, s_2)\)</span> are both Nash equilibria.</p>
<p><strong>Proof:</strong> Recall in a zero-sum game, we have for any strategy profile <span class="math inline">\(s\)</span>, <span class="math inline">\(u_1(s) + u_2(s) = 0\)</span>. We can see then that <span class="math display">\[ u_1(s_1, s_2) = -u_2(s_1, s_2) \le -u_2(s_1, s'_2) = u_1(s_1, s'_2) \le u_1(s'_1, s'_2) \]</span> By symmetry, we get <span class="math inline">\(u_1(s_1, s_2) \ge u_1(s'_1, s'_2)\)</span>, and so <span class="math inline">\(u_i(s_1, s_2) = u_i(s'_1, s'_2)\)</span>.</p>
<p>Consider the profile <span class="math inline">\((s_1, s'_2)\)</span>, and let <span class="math inline">\(s''_1\)</span> be another strategy that agent <span class="math inline">\(1\)</span> could take. Using the above equality we can see <span class="math display">\[ u_1(s''_1, s'_2) \le u_1(s'_1, s'_2) = -u_2(s'_1, s'_2) \]</span> since <span class="math inline">\((s'_1, s'_2)\)</span> is a Nash equilibrium. By the above, <span class="math inline">\(u_2(s'_1, s'_2) = u_2(s_1, s_2)\)</span> so <span class="math display">\[ -u_2(s'_1, s'_2) = -u_2(s_1, s_2) \le -u_2(s_1, s'_2) = u_1(s_1, s'_2)\]</span> Combining with above, we see that <span class="math inline">\(s_1\)</span> is the best response for agent <span class="math inline">\(1\)</span> for the strategy <span class="math inline">\(s'_2\)</span> of agent <span class="math inline">\(2\)</span>. A symmetric argument for a strategy <span class="math inline">\(s''_2\)</span> taken by agent <span class="math inline">\(2\)</span> shows that <span class="math inline">\((s_1, s'_2)\)</span> is a Nash equilibrium. The case for <span class="math inline">\((s'_1, s_2)\)</span> follows similarly. <span class="math inline">\(\square\)</span></p>
<p>The second property is one that we proved in the proof above: in a zero-sum game, the expected payoff to each player is the same for every Nash equilibrium.</p>
<h3 id="computing-equilibria">computing equilibria</h3>
<p>Let’s restrict for now to the case of two-player zero-sum games. How would we compute the Nash equilibria of such a game? We first introduce the <em>maxmin</em> strategy for an agent.</p>
<p>Suppose we have a conservative agent– they would want to maximize their expected utility regardless of what the other agents do. In particular, they would seek a strategy that maximizes their expected utility in the <em>worst case scenario</em> that the other agents act to minimize their payoff. This is the <strong>maxmin strategy</strong> <span class="math display">\[ \underline{s}_i = \arg\max_{s_i}\min_{s_{-i}}{u_i(s_i, s_{-i})} \]</span> The payoff from this strategy is the <strong>maxmin value</strong> <span class="math display">\[ \underline{v}_i = \max_{s_i}\min_{s_{-i}}{u_i(s_i, s_{-i})} \]</span> Analogously, an agent could seek a strategy to maximally punish the payoffs of their opponents, regardless of the damage to themselves. This leads to the <strong>minmax strategy</strong> and value <span class="math display">\[ \overline{v}_i = \min_{s_i}\max_{s_{-i}}{u_i(s_i, s_{-i})} \]</span> Note that in general, <span class="math inline">\(\underline{v}_i \le \overline{v}_i\)</span>. However, when we’re in a finite two-player zero-sum game, we can prove something even stronger. Let <span class="math inline">\((s^*_i, s^*_{-i})\)</span> be a Nash equilibrium and <span class="math inline">\(v^*_i\)</span> be the expected utility of agent <span class="math inline">\(i\)</span> in this equilibrium.</p>
<p>First, we observe that <span class="math inline">\(v^*_i \ge \underline{v_i}\)</span>. This is because if <span class="math inline">\(v^*_i &lt; \underline{v}_i\)</span> then agent <span class="math inline">\(i\)</span> could gain greater utility by using their maxmin strategy instead, which is a contradiction to <span class="math inline">\(s^*\)</span> being a Nash equilibrium. In an equilibrium, all agents acts according to their best response to the other’s strategies: <span class="math display">\[ v^*_{-i} = \max_{s_{-i}}{u_{-i}(s^*_i, s_{-i})} \]</span> As we’re in a zero-sum game, <span class="math inline">\(v^*_i = -v^*_{-i}\)</span> and <span class="math inline">\(u_i=-u_{-i}\)</span>, so <span class="math display">\[ \begin{aligned} v^*_i &amp;= -v^*_{-i} \\ &amp;= -\max_{s_{-i}}{u_{-i}(s^*_i, s_{-i})} \\
&amp;= \min_{s_{-i}}{-u_{-i}(s^*_i, s_{-i})} \\
&amp;= \min_{s_{-i}}{u_i(s^*_i, s_{-i})}
\end{aligned}
\]</span> By definition then, <span class="math display">\[ \underline{v}_i = \max_{s_i}\min_{s_{-i}}{u_i(s_i, s_{-i})} \ge \min_{s_{-i}}{u_i(s^*_i, s_{-i})} = v^*_i \]</span> Hence <span class="math inline">\(\underline{v}_i = v^*_i\)</span> and so our maxmin strategy is actually a Nash equilibrium! This is the <strong>minimax theorem</strong> proven by von Neumann in 1928.</p>
<p>In general, computing the Nash equilibria of a general normal-form game is computationally difficult (even from a complexity theory standpoint)! More precisely, a few years ago Babichenko-Rubinstein <a href="https://arxiv.org/abs/1608.06580">show</a> that there is no guaranteed method for players to find even an approximate Nash equilibrium unless they tell each other almost everything about their preferences.</p>
<p>We instead focus on a different solution type that is more computationally efficient, the <strong>correlated equilibria</strong>, which is a strict generalization of Nash equilibrium. It helps to start with an example:</p>
<pre><code>        STOP         GO
     -------------------------
STOP | (0, 0) |    (0, 1)    |
GO   | (1, 0) | (-100, -100) |
     -------------------------</code></pre>
<p>In this <strong>traffic light game</strong>, each player has two actions– stop or go. It’s clear that there are two pure-strategy Nash equilibria, the off-diagonals <code>(GO, STOP)</code> and <code>(STOP, GO)</code>. However, these are not ideal, as only one person benefits in either situation.</p>
<p>Let’s try and compute a mixed strategy Nash equilibrium for this game. Let the strategy of the first player be <span class="math inline">\(\sigma=(p, 1-p)\)</span> where <span class="math inline">\(p\)</span> is the probability of <code>GO</code>. How do we choose <span class="math inline">\(p\)</span>? It should be chosen such that if player 2 plays with a best response, player 1 plays in such a way that player 2 is indifferent between their actions, that is</p>
<p><span class="math display">\[  u_2(\sigma, \text{GO}) = u_2(\sigma, \text{STOP}) \]</span></p>
<p>If this were to not hold, e.g. <span class="math inline">\(u_2(\sigma, \text{GO}) &gt; u_2(\sigma, \text{STOP})\)</span>, then player 2 would have a better time choosing <code>GO</code> more often, contradicting that they play with a best response. Computing out the expected utilities, we have</p>
<p><span class="math display">\[ 1\cdot (1-p) - 100p = 0 \]</span></p>
<p>which implies <span class="math inline">\(p = 1/101\)</span>. Hence in a mixed Nash equilibrium, both players go at a traffic stop with absurdly low probability! This is obviously very unideal.</p>
<p>Luckily, this isn’t how traffic stops at intersections work in the real world. In this game-theoretic setting, each mixed strategy that goes into a Nash equilibrium gives a probability distribution over actions for each player– however, each action ends up choosing their action independently, without any communication between them. In real-life, we have <em>traffic lights</em>, which act as a signal to both players which suggests their action. This <em>correlates</em> the action of each player without fixing them to a single pure strategy.</p>
<p>This gives us the idea of a <strong>correlated equilibrium</strong>: a distribution <span class="math inline">\(\mathcal{D}\)</span> over action profiles <span class="math inline">\(A\)</span> is a correlated equilbrium if for each player <span class="math inline">\(i\)</span> and action <span class="math inline">\(a^*_i\)</span>, we have</p>
<p><span class="math display">\[ \mathbf{E}_{a\sim\mathcal{D}}[u_i(a)] \ge \mathbf{E}_{a\sim\mathcal{D}}[u_i(a^*_i, a_{-i})|a_i] \]</span></p>
<p>that is, after a profile <span class="math inline">\(a\)</span> is drawn, playing <span class="math inline">\(a_i\)</span> is a best response for player <span class="math inline">\(i\)</span> conditioned on seeing <span class="math inline">\(a_i\)</span>, given that everyone else will play according to <span class="math inline">\(a\)</span>. In the traffic light game, conditioned on seeing <code>STOP</code>, a player knows the other player sees <code>GO</code> so their best response is to <code>STOP</code>, and vice versa.</p>
<p>Correlated equilibria generalize Nash equilibrium– any Nash equilibrium is a correlated one, in the case that each player’s actions are drawn from independent distributions, i.e.</p>
<p><span class="math display">\[ \Pr(a|\sigma) = \prod_{i\in N} \Pr(a_i|\sigma) \]</span></p>
<p>Despite looking as complex the definition of a Nash equilibrium, correlated equilibria are much more tractable to compute: they can be computed using no-regret learning, i.e. regret minimization. We discuss this next.</p>
<h3 id="regret-minimization">regret minimization</h3>
<p>In this section we give the basics of regret minimization, treating it initially from the perspective of online optimization. Let <span class="math inline">\(\mathcal{X}\)</span> be a space of strategies for a given agent. We consider decision processes in which at time <span class="math inline">\(t=1,2,...\)</span> the agent (player) will play an action <span class="math inline">\(x_t\in\mathcal{X}\)</span>, receive “feedback” from the environment (the game, other players, etc) and then use it to formulate a response <span class="math inline">\(x_{t+1}\in\mathcal{X}\)</span>.</p>
<p>Given <span class="math inline">\(\mathcal{X}\)</span> and a set <span class="math inline">\(\Phi\)</span> of linear transforms <span class="math inline">\(\phi:\mathcal{X}\rightarrow\mathcal{X}\)</span>, a <strong><span class="math inline">\(\Phi\)</span>-regret minimizer</strong> for <span class="math inline">\(\mathcal{X}\)</span> is a model for a decision maker that repeatedly interacts with the environment via the API</p>
<ul>
<li><code>NextStrategy</code> which outputs a strategy <span class="math inline">\(x_t\in\mathcal{X}\)</span> at decision time <span class="math inline">\(t\)</span>.</li>
<li><code>ObserveUtility</code>(<span class="math inline">\(\ell^t\)</span>) which updates the decision-making process of the agent, in the form of a linear utility function (or vector) <span class="math inline">\(\ell^t:\mathcal{X}\rightarrow\mathbf{R}\)</span>.</li>
</ul>
<p>The quality metric of our minimizer is given by <strong>cumulative <span class="math inline">\(\Phi\)</span>-regret</strong></p>
<p><span class="math display">\[ R^T_\Phi = \max_{\hat{\phi}\in\Phi}\left\{\sum_{t=1}^T\left(\ell^t(\hat{\phi}(x_t))-\ell^t(x_t)\right)\right\} \]</span></p>
<p>where the interior term <span class="math inline">\(\ell^t(\hat{\phi}(x_t))-\ell^t(x_t)\)</span> is the <em>regret</em> at time <span class="math inline">\(t\)</span> of not changing our strategy by <span class="math inline">\(\hat{\phi}\)</span>. The <strong>goal</strong> of a <span class="math inline">\(\Phi\)</span>-regret minimizer is to guarantee its <span class="math inline">\(\Phi\)</span>-regret grows <strong>asymptotically sublinearly</strong> as <span class="math inline">\(T\rightarrow\infty\)</span>.</p>
<p>Intuitively, the class <span class="math inline">\(\Phi\)</span> of functions constraints the kind of “regret” we can feel– a function <span class="math inline">\(\phi\in\Phi\)</span> tells us how we could have hypothetically swapped out our choice of action for a more optimal one, and the regret measures how much better that choice could have been for our decision. In this way, we want to <em>minimize</em> our regret by more often choosing optimal actions.</p>
<p>For an example of a class <span class="math inline">\(\Phi\)</span>, we can take <span class="math inline">\(\Phi=\{\text{all linear maps }\mathcal{X}\rightarrow\mathcal{X}\}\)</span>. Then the notion of <span class="math inline">\(\Phi\)</span>-regret is called <strong>swap regret</strong>. Intuitively, it is the measure of how much a player can improve by switching any action we choose to the best decision possible in hindsight.</p>
<p><strong>Note:</strong> If we restrict our choice of <span class="math inline">\(\Phi\)</span> to be <span class="math inline">\(\Phi=\{\phi_{a\rightarrow b}\}_{a,b\in\mathcal{X}}\)</span> where</p>
<p><span class="math display">\[  
\begin{equation}
    \phi_{a\rightarrow b}(x) = 
    \begin{cases}
        x &amp; \text{if } x\neq a\\
        b &amp; \text{if } x=a
    \end{cases}
\end{equation}
\]</span></p>
<p>then we get the closely related concept of <strong>internal regret</strong>.</p>
<p>A very special case of <span class="math inline">\(\Phi\)</span>-regret comes when <span class="math inline">\(\Phi\)</span> is the set of constant functions.</p>
<p><strong>Defn:</strong> (Regret minimizer) An <strong>external regret minimizer</strong> for <span class="math inline">\(\mathcal{X}\)</span> is a <span class="math inline">\(\Phi^{\text{const}}\)</span>-regret minimizer for</p>
<p><span class="math display">\[ \Phi^{\text{const}} = \left\{\phi_{\hat{x}}: x\mapsto \hat{x}\right\}_{\hat{x}\in\mathcal{X}} \]</span></p>
<p>The <span class="math inline">\(\Phi^{\text{const}}\)</span>-regret is just called <strong>external regret</strong></p>
<p><span class="math display">\[ R^T = \max_{\hat{x}\in\mathcal{X}}\left\{\sum_{t=1}^T\left(\ell^t(\hat{x}) - \ell^t(x_t)\right)\right\} \]</span></p>
<p>Regret minimizers are useful in helping us find best responses in various game-theoretic settings. Suppose we’re playing an <span class="math inline">\(n\)</span>-player game where players <span class="math inline">\(1,...,n-1\)</span> play stochastically, i.e. at each time <span class="math inline">\(t\)</span>, we get a strategy <span class="math inline">\(x^{(i)}_t\in\mathcal{X}^{(i)}\)</span> with</p>
<p><span class="math display">\[ \mathbf{E}[x^{(i)}_t] = \bar{x}^{(i)}\in\mathcal{X}^{(i)} \]</span></p>
<p>We let player <span class="math inline">\(n\)</span> picks strategies according to an algorithm that guarantees sublinear external regret (i.e. via a <span class="math inline">\(\Phi^{\text{const}}\)</span>-regret minimizer), where the utility function is given by the multilinear payoff functional</p>
<p><span class="math display">\[ \ell^t(x^{(n)}) = u_n(x^{(1)}_t, x^{(2)}_t,..., x^{(n-1)}_t, x^{(n)}) \]</span></p>
<p>The <em>claim</em> is that the average of player <span class="math inline">\(n\)</span>’s strategies converges to the best response to <span class="math inline">\(\bar{x}^{(1)},...,\bar{x}^{(n-1)}\)</span>:</p>
<p><span class="math display">\[ \frac{1}{T}\sum_{t=1}^T x^{(n)}_t \xrightarrow{T\rightarrow\infty} \argmax_{\hat{x}^{(n)}\in\mathcal{X}^{(n)}}\left\{u_n(\bar{x}^{(1)},...,\bar{x}^{(n-1)}, \hat{x}^{(n)})\right\} \]</span></p>
<p>To see this, note that by multilinearity,</p>
<p><span class="math display">\[
\begin{align*}
    R^T &amp;= \max_{\hat{x}\in\mathcal{X}^{(n)}}\left\{\sum_{t=1}^T\left(u_n(x^{(1)}_t,...,\hat{x})-u_n(x^{(1)}_t,...,x^{(n)}_t)\right)\right\} \\
        &amp;= \max_{\hat{x}\in\mathcal{X}^{(n)}}\left\{\sum_{t=1}^T u_n(x^{(1)}_t,...,\hat{x}-x^{(n)}_t)\right\}
\end{align*}
\]</span></p>
<p>and as <span class="math inline">\(\frac{R^T}{T}\to 0\)</span> by sublinearity of regret, this proves that <span class="math inline">\(\frac{1}{T}\sum_{t=1}^T x_t^{(n)}\to\hat{x}\)</span>.</p>
<p>Our main use of regret minimization is to compute (correlated) equilibria. Let’s restrict to the case of two-person zero-sum games. Given strategies <span class="math inline">\(x\in\mathcal{X}\subset\mathbf{R}^n, y\in\mathcal{Y}\subset\mathbf{R}^m\)</span> and a linear payoff matrix <span class="math inline">\(A\in\operatorname{Mat}_{n,m}\)</span> for player 1, the <strong>utility</strong> of player 1 is given by <span class="math inline">\(x^\top A y\)</span>.</p>
<p>We seek a Nash equilibrium, which we have prove is a minimax solution</p>
<p><span class="math display">\[ \max_{x\in\mathcal{X}}\min_{y\in\mathcal{Y}} x^\top A y \]</span></p>
<p>i.e. we want to use regret minimization to compute a bilinear saddle point.</p>
<p><strong>Rmk:</strong> In two-player zero-sum games, regret minimization processes will converge to Nash equilibria, since in this situation there are no extra correlated equilibria. This intuitively makes sense since zero-sum games are purely adversarial and there is no “gain” from cooperation in these situations. For some extended thoughts on this, see this <a href="https://www.kellogg.northwestern.edu/research/math/papers/45.pdf">paper</a> of Rosenthal.</p>
<p>To compute these equilibria, we create a regret minimizer <span class="math inline">\(\mathcal{R}_\mathcal{X}, \mathcal{R}_\mathcal{Y}\)</span> per player with utility functions</p>
<p><span class="math display">\[
\begin{align*}
    \ell^t_\mathcal{X} &amp;: x\mapsto (Ay_t)^\top x \\
    \ell^t_\mathcal{Y} &amp;: y\mapsto -(A^\top x_t)^\top y_t
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(x_t, y_t\)</span> are strategies generated by the regret minimizers at time <span class="math inline">\(t\)</span>. This idea is called <strong>self-play</strong>.</p>
<p>Let <span class="math inline">\(\gamma\)</span> be the <strong>saddle point gap</strong></p>
<p><span class="math display">\[ 
\begin{align*}
    \gamma(x,y) &amp;= \left(\max_{\hat{x}\in\mathcal{X}} \hat{x}^\top Ay - x^\top Ay\right) + \left(x^\top Ay - \min_{\hat{y}\in\mathcal{Y}} x^\top A\hat{y}\right) \\
    &amp;= \max_{\hat{x}\in\mathcal{X}} \hat{x}^\top Ay - \min_{\hat{y}\in\mathcal{Y}} x^\top A\hat{y}
\end{align*}    
\]</span></p>
<p>where the left term is the best response payoff to <span class="math inline">\(t\)</span> and right right term is the best response payoff to <span class="math inline">\(x\)</span> (both in the perspective of player 1). If <span class="math inline">\(\gamma(x,y)=0\)</span> then the strategy profile <span class="math inline">\(\sigma=(x,y)\)</span> is a Nash equilibrium.</p>
<p><strong>Thm:</strong> As <span class="math inline">\(T\to\infty\)</span>, the average strategies <span class="math inline">\(\bar{x}=\frac{1}{T}\sum_{t=1}^T x_t\)</span> and <span class="math inline">\(\bar{y}=\frac{1}{T}\sum_{t=1}^T y_t\)</span> approaches a Nash equilibrium.</p>
<p><em>Proof:</em> By definition, we can write</p>
<p><span class="math display">\[
\begin{align*}
    \frac{1}{T}\left(R^T_\mathcal{X} + R^T_\mathcal{Y}\right)
    &amp;= \frac{1}{T}\max_{\hat{x}\in\mathcal{X}}\left\{\sum_{t=1}^T\left(\ell_\mathcal{X}^t(\hat{x})-\ell_\mathcal{X}^t(x_t)\right)\right\}
        + \frac{1}{T}\max_{\hat{y}\in\mathcal{Y}}\left\{\sum_{t=1}^T\left(\ell_\mathcal{Y}^t(\hat{Y})-\ell_\mathcal{Y}^t(y_t)\right)\right\} \\
    &amp;= \frac{1}{T}\max_{\hat{x}\in\mathcal{X}}\left\{\sum_{t=1}^T \ell^t_\mathcal{X}(\hat{x})\right\}
        + \frac{1}{T}\max_{\hat{y}\in\mathcal{Y}}\left\{\sum_{t=1}^T \ell^t_\mathcal{Y}(\hat{y})\right\}
\end{align*}
\]</span></p>
<p>as <span class="math inline">\(\ell^t_\mathcal{X}(x_t) + \ell_\mathcal{Y}^t(y_t) = (Ay_t)^\top x_t - (A^\top x_t)^\top y_t = 0\)</span>. Continuing,</p>
<p><span class="math display">\[
\begin{align*}
    &amp;= \frac{1}{T}\max_{\hat{x}\in\mathcal{X}}\left\{\sum_{t=1}^T \hat{x}^\top Ay_t\right\}
        + \frac{1}{T}\max_{\hat{y}\in\mathcal{Y}}\left\{\sum_{t=1}^T (-x_t^\top A\hat{y} \right\} \\
    &amp;= \max_{\hat{x}\in\mathcal{X}}\hat{x}^\top A\bar{y} - \min_{\hat{y}\in\mathcal{Y}} \bar{x}^\top A\hat{y} \\
    &amp;= \gamma(\bar{x}, \bar{y})
\end{align*}
\]</span></p>
<p>By sublinearity of the regret minimizers, <span class="math inline">\(\frac{R^T_\mathcal{X}+R^T_\mathcal{Y}}{T}\to 0\)</span>. <span class="math inline">\(\square\)</span></p>
<h3 id="minimax-via-regret">minimax via regret</h3>
<p>In this section, we get another glimpse into the power of regret minimizers in optimization problems by reproving the minimax theorem. We first start with the easy part– note that since</p>
<p><span class="math display">\[ \min_{y\in\mathcal{Y}} x^\top Ay \le x^\top Ay \]</span></p>
<p>we get automatically that applying <span class="math inline">\(\max_{x\in\mathcal{X}}\)</span> on both sides is also true. Since the right side is then still a function of <span class="math inline">\(y\)</span>, we can minimize it and get</p>
<p><span class="math display">\[ \max_{x\in\mathcal{X}}\min_{y\in\mathcal{Y}} x^\top Ay \le \min_{y\in\mathcal{Y}}\max_{x\in\mathcal{X}} x^\top Ay \]</span></p>
<p>This is <strong>weak duality</strong>.</p>
<p>To prove the minimax theorem, we need to prove the inequality in the other direction. As in the previous regret learning situation, we play a repeated game between a regret minimizer an the environment: <span class="math inline">\(\mathcal{R}_\mathcal{X}\)</span> chooses a strategy <span class="math inline">\(x_t\in\mathcal{X}\)</span> and the environment plays <span class="math inline">\(y_t\in\mathcal{Y}\)</span> such that <span class="math inline">\(y_t\)</span> is a best response:</p>
<p><span class="math display">\[ y_t \in\argmin_{y\in\mathcal{Y}} x_t^\top Ay \]</span></p>
<p>The utility function observed by <span class="math inline">\(\mathcal{R}_\mathcal{X}\)</span> at each time <span class="math inline">\(t\)</span> is given by</p>
<p><span class="math display">\[ \ell^t_\mathcal{X}: x\mapsto x^\top Ay_t \]</span></p>
<p>We assume that <span class="math inline">\(\mathcal{R}_\mathcal{X}\)</span> gives sublinear regret in the worst case. Let <span class="math inline">\(\bar{x}^T=\frac{1}{T}\sum_{t=1}^T x_t\)</span> and <span class="math inline">\(\bar{y}^T=\frac{1}{T}\sum_{t=1}^T y_t\)</span> be the average strategies up to time <span class="math inline">\(T\)</span>. For all <span class="math inline">\(t\)</span>,</p>
<p><span class="math display">\[ \max_{x\in\mathcal{X}}\min_{y\in\mathcal{Y}} x^\top Ay \ge \frac{1}{T}\min_{y\in\mathcal{Y}}\sum_{t=1}^T x_t^\top Ay \text{ as each } \min_{y\in\mathcal{Y}} x^\top_t Ay\le \max_{x\in\mathcal{X}}\min_{y\in\mathcal{Y}} x^\top Ay \]</span></p>
<p>As each <span class="math inline">\(y_t\)</span> is the best response of the environment to the strategy <span class="math inline">\(x_t\)</span>, we have</p>
<p><span class="math display">\[ \min_{y\in\mathcal{Y}} x_t^\top Ay = x^\top_t Ay_t \]</span></p>
<p>so that</p>
<p><span class="math display">\[ \max_{x\in\mathcal{X}}\min_{y\in\mathcal{Y}} x^\top Ay \ge \frac{1}{T}\min_{y\in\mathcal{Y}}\sum_{t=1}^T x_t^\top Ay \ge \frac{1}{T}\sum_{t=1}^T x^\top_t Ay_t \]</span></p>
<p>By definition of regret <span class="math inline">\(R^T_\mathcal{X}=\max_{\hat{x}\in\mathcal{X}}\left\{\sum_{t=1}^T\left(\hat{x}^\top Ay_t-x^\top_t Ay_t\right)\right\}\)</span> so</p>
<p><span class="math display">\[
\begin{align*}
    \frac{1}{T}\sum_{t=1}^T x_t^\top Ay_t &amp;\ge \frac{1}{T}\max_{\hat{x}\in\mathcal{X}}\sum_{t=1}^T \hat{x}^\top Ay_t - \frac{R^T_\mathcal{X}}{T} \\
    &amp;\ge \min_{y\in\mathcal{Y}}\max_{x\in\mathcal{X}} x^\top Ay - \frac{R^T_\mathcal{X}}{T}
\end{align*}
\]</span></p>
<p>By sublinearity, <span class="math inline">\(\frac{R^T_\mathcal{X}}{T}\to 0\)</span>, so we see that</p>
<p><span class="math display">\[ \max_{x\in\mathcal{X}}\min_{y\in\mathcal{Y}} x^\top Ay \ge \min_{y\in\mathcal{Y}}\max_{x\in\mathcal{X}} x^\top Ay \]</span></p>
<p>This proves minimax.</p>
<h3 id="regret-matching">regret matching</h3>
<p>In order to perform regret minimization, we require computable algorithms to generate decisions with sublinear regret guarantees, i.e. regret minimizers for domain sets <span class="math inline">\(\mathcal{X}\)</span>. A fundamental no-regret algorithm is given by <strong>regret matching</strong>, which gives a sublinear regret minimizer for probability simplices</p>
<p><span class="math display">\[ \Delta^n = \left\{(x_1,...,x_n)\in\mathbf{R}^n_{\ge 0} : x_1 +...+ x_n = 1\right\} \]</span></p>
<p>which model one-shot decision processes (such as agent actions in normal-form games).</p>
<p>Remember that a regret minimizer for <span class="math inline">\(\Delta^n\)</span> is given by</p>
<ul>
<li><code>NextElement</code> outputting an element <span class="math inline">\(x_t\in\Delta^n\)</span>, and</li>
<li><code>ObserveUtility</code>(<span class="math inline">\(\ell^t\)</span>) computes environment feedback on this action <span class="math inline">\(x_t\)</span> given a linear utility vector <span class="math inline">\(\ell^t\in\mathbf{R}^n\)</span> that evaluates how good <span class="math inline">\(x_t\)</span> was. Note that we are overloading notation here, as <span class="math inline">\(\ell^t\)</span> is really a function</li>
</ul>
<p><span class="math display">\[ \ell^t:\Delta^n\to\mathbf{R}\text{ given by } x\mapsto\langle\ell^t, x\rangle \]</span></p>
<p>This minimizer <span class="math inline">\(\mathcal{R}\)</span> should have its cumulative regret</p>
<p><span class="math display">\[ R^T = \max_{\hat{x}\in\Delta^n}\left\{ \sum_{t=1}^T\left(\langle\ell^t,\hat{x}\rangle - \langle\ell^t,x_t\rangle\right)\right\} \]</span></p>
<p>grow sublinearly as <span class="math inline">\(T\to\infty\)</span>, regardless of the utility vectors <span class="math inline">\(\ell^t\)</span> chosen by the environment.</p>
<p>We describe the regret matching algorithm, along with a Python implementation. Recall that to implement a regret minimizer, we need to complete a specific API:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb3-2" title="2"></a>
<a class="sourceLine" id="cb3-3" title="3"><span class="kw">class</span> RegretMinimizer:</a>
<a class="sourceLine" id="cb3-4" title="4">    <span class="at">@abstractmethod</span></a>
<a class="sourceLine" id="cb3-5" title="5">    <span class="kw">def</span> next_strategy(<span class="va">self</span>) <span class="op">-&gt;</span> np.ndarray:</a>
<a class="sourceLine" id="cb3-6" title="6">        <span class="cf">raise</span> <span class="pp">NotImplementedError</span></a>
<a class="sourceLine" id="cb3-7" title="7">    </a>
<a class="sourceLine" id="cb3-8" title="8">    <span class="at">@abstractmethod</span></a>
<a class="sourceLine" id="cb3-9" title="9">    <span class="kw">def</span> observe_utility(<span class="va">self</span>, utility_vec: np.ndarray):</a>
<a class="sourceLine" id="cb3-10" title="10">        <span class="cf">raise</span> <span class="pp">NotImplementedError</span></a></code></pre></div>
<p>Regret matching will be a specific instance of a regret minimizer. Such decision-generating agents have some internal state that allows it to update its strategies over time (i.e. learning). At time 0, for regret matching we set a cumulative <em>regret</em> vector <span class="math inline">\(r_0\in\mathbf{R}^n\)</span> to <span class="math inline">\(\mathbf{0}\)</span> and we set an initial (uniform) strategy <span class="math inline">\(x_0=\left(\frac{1}{n},...,\frac{1}{n}\right)\in\mathbf{R}^n\)</span> where <span class="math inline">\(n\)</span> here is the number of actions <span class="math inline">\(|\mathcal{X}|\)</span> the agent following this strategy can make.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1"><span class="kw">class</span> RegretMatcher(RegretMinimizer):</a>
<a class="sourceLine" id="cb4-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_actions):</a>
<a class="sourceLine" id="cb4-3" title="3">        <span class="va">self</span>.num_actions <span class="op">=</span> num_actions</a>
<a class="sourceLine" id="cb4-4" title="4">        <span class="va">self</span>.regret_sum <span class="op">=</span> np.zeros(num_actions)</a>
<a class="sourceLine" id="cb4-5" title="5">        <span class="va">self</span>.current_strategy <span class="op">=</span> np.zeros(num_actions)</a>
<a class="sourceLine" id="cb4-6" title="6">        <span class="va">self</span>.last_strategy <span class="op">=</span> np.zeros(num_actions)</a></code></pre></div>
<p>Suppose at time <span class="math inline">\(t\)</span> we are given a strategy <span class="math inline">\(x_t\in\mathbf{R}^n\)</span>. How do we update our regret vector <span class="math inline">\(r_{t-1}\)</span> to <span class="math inline">\(r_t\)</span> and use it to generate the next strategy? The intuition behind regret matching is that we should choose the actions that we regret not having chosen in the past more often.</p>
<p>Given <span class="math inline">\(r_{t-1}\)</span>, let <span class="math inline">\(\theta_t = [r_{t-1}]^+\)</span> to be the vector gotten by setting any negative terms in the vector to 0. In this sense, negative regret is useless to us, since we don’t want to disincentivize choosing an action that already is giving us benefits. However, <span class="math inline">\(\theta_t\)</span> may no longer be in <span class="math inline">\(\Delta^n\)</span>, but we can force it by normalizing. So we take as our next strategy</p>
<p><span class="math display">\[ x_t = \frac{\theta_t}{\|\theta_t\|_1} \]</span></p>
<p>In code,</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1">    <span class="kw">def</span> next_strategy(<span class="va">self</span>) <span class="op">-&gt;</span> np.ndarray:</a>
<a class="sourceLine" id="cb5-2" title="2">        regrets <span class="op">=</span> np.copy(<span class="va">self</span>.regret_sum)</a>
<a class="sourceLine" id="cb5-3" title="3">        regrets[regrets <span class="op">&lt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb5-4" title="4">        normalizing_sum <span class="op">=</span> np.<span class="bu">sum</span>(regrets)</a>
<a class="sourceLine" id="cb5-5" title="5">        <span class="cf">if</span> normalizing_sum <span class="op">&gt;</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb5-6" title="6">            strategy <span class="op">=</span> regrets <span class="op">/</span> normalizing_sum</a>
<a class="sourceLine" id="cb5-7" title="7">        <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb5-8" title="8">            <span class="co"># default to uniform</span></a>
<a class="sourceLine" id="cb5-9" title="9">            strategy <span class="op">=</span> np.repeat(<span class="dv">1</span> <span class="op">/</span> <span class="va">self</span>.num_actions, <span class="va">self</span>.num_actions)</a>
<a class="sourceLine" id="cb5-10" title="10"></a>
<a class="sourceLine" id="cb5-11" title="11">        <span class="va">self</span>.current_strategy <span class="op">=</span> strategy</a>
<a class="sourceLine" id="cb5-12" title="12">        <span class="va">self</span>.strategy_sum <span class="op">+=</span> strategy</a>
<a class="sourceLine" id="cb5-13" title="13">        <span class="cf">return</span> strategy</a></code></pre></div>
<p>Given a feedback vector <span class="math inline">\(\ell^t\)</span>, we want to now update our internal state in order to generate better strategies. Often, we can interpret our feedback utility vector from the environment as</p>
<p><span class="math display">\[ \ell^t_a = \text{the payoff gotten if we purely chose action a} \]</span></p>
<p>Consider the term</p>
<p><span class="math display">\[ \alpha_t = \ell^t - \langle\ell^t, x_t\rangle \mathbf{1} \]</span></p>
<p>where <span class="math inline">\(\mathbf{1}\)</span> is the vector <span class="math inline">\((1,...,1)\in\mathbf{R}^n\)</span>. Interpreting <span class="math inline">\(\langle\ell^t, x_t\rangle\)</span> as the expected utility of <span class="math inline">\(x_t\)</span>, we can interpret the vector <span class="math inline">\(\alpha_t\)</span> as</p>
<p><span class="math display">\[ \alpha_{t,a} = \text{the regret of not purely choosing action a} \]</span></p>
<p>To update our cumulative regret, we fold this into the mix: <span class="math inline">\(r_t = r_{t-1} + \alpha_t\)</span>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" title="1">    <span class="kw">def</span> observe_utility(<span class="va">self</span>, utility_vector: np.ndarray):</a>
<a class="sourceLine" id="cb6-2" title="2">        expected_utility <span class="op">=</span> np.dot(utility_vector, <span class="va">self</span>.current_strategy)</a>
<a class="sourceLine" id="cb6-3" title="3">        regrets <span class="op">=</span> utility_vector <span class="op">-</span> expected_utility</a>
<a class="sourceLine" id="cb6-4" title="4">        <span class="va">self</span>.regret_sum <span class="op">+=</span> regrets</a></code></pre></div>
<p>Performing this iterative gives the regret matching algorithm.</p>
<p><strong>Rmk:</strong> If we take <span class="math inline">\(r_t=[r_{t-1}+\alpha_t]^+\)</span> instead, we get the <span class="math inline">\(\text{regret matching}^+\)</span> algorithm.</p>
<h3 id="composition-and-swap-regret">composition and swap-regret*</h3>
<p><strong>This section could be skipped on a first reading!</strong></p>
<p>Now that we have a regret minimizer for <span class="math inline">\(\Delta^n\)</span>, can we build regret minimizers for other spaces? Since we can hope to build up our spaces as compositions of probability simplices, if we had rules to combine regret minimizers for certain algebraic operations we could generically build regret minimizers for a whole range of domain spaces <span class="math inline">\(\mathcal{X}\)</span>.</p>
<p>Suppose <span class="math inline">\(\mathcal{R}_\mathcal{X}\)</span>, <span class="math inline">\(\mathcal{R}_\mathcal{Y}\)</span> are regret minimizers for <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{Y}\)</span> respectively. Trivially, we get a regret minimizer for <span class="math inline">\(\mathcal{X}\times\mathcal{Y}\)</span> by</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" title="1"><span class="kw">class</span> Product(RegretMinimizer):</a>
<a class="sourceLine" id="cb7-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, r_x: RegretMinimizer, r_y: RegretMinimizer):</a>
<a class="sourceLine" id="cb7-3" title="3">        <span class="va">self</span>.r_x, <span class="va">self</span>.r_y <span class="op">=</span> r_x, r_y</a>
<a class="sourceLine" id="cb7-4" title="4"></a>
<a class="sourceLine" id="cb7-5" title="5">    <span class="kw">def</span> next_strategy(<span class="va">self</span>) <span class="op">-&gt;</span> np.ndarray:</a>
<a class="sourceLine" id="cb7-6" title="6">        x_t <span class="op">=</span> <span class="va">self</span>.r_x.next_strategy()</a>
<a class="sourceLine" id="cb7-7" title="7">        y_t <span class="op">=</span> <span class="va">self</span>.r_y.next_strategy()</a>
<a class="sourceLine" id="cb7-8" title="8">        <span class="cf">return</span> np.concatenate([x_t, y_t], axis<span class="op">=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb7-9" title="9"></a>
<a class="sourceLine" id="cb7-10" title="10">    <span class="kw">def</span> observe_utility(<span class="va">self</span>, utility_vector: np.ndarray):</a>
<a class="sourceLine" id="cb7-11" title="11">        num_actions_x <span class="op">=</span> <span class="va">self</span>.r_x.num_actions</a>
<a class="sourceLine" id="cb7-12" title="12">        <span class="va">self</span>.r_x.observe_utility(utility_vector[:num_actions_x])</a>
<a class="sourceLine" id="cb7-13" title="13">        <span class="va">self</span>.r_y.observe_utility(utility_vector[num_actions_x:])</a></code></pre></div>
<p>It is clear that</p>
<p><span class="math display">\[  R^T_{\mathcal{X}\times\mathcal{Y}} = R^T_\mathcal{X} + R^T_\mathcal{Y} \]</span></p>
<p>so if <span class="math inline">\(\mathcal{R}_\mathcal{X}\)</span>, <span class="math inline">\(\mathcal{R}_\mathcal{Y}\)</span> have sublinear regret, so does <span class="math inline">\(\mathcal{R}_{\mathcal{X}\times\mathcal{Y}}\)</span>.</p>
<p>Less trivially, consider the algebraic operation given by the convex hull <span class="math inline">\(\operatorname{conv}(\mathcal{X}, \mathcal{Y})\)</span>. Here we are assuming that <span class="math inline">\(\mathcal{X},\mathcal{Y}\subset\mathbf{R}^n\)</span>. Along with regret minimizers <span class="math inline">\(\mathcal{R}_\mathcal{X}\)</span>, <span class="math inline">\(\mathcal{R}_\mathcal{Y}\)</span>, we also need a regret minimizer for the simplex <span class="math inline">\(\Delta^2\)</span> (which we can luckily use the regret matching algorithm above)!</p>
<p>Then we get a regret minimizer for <span class="math inline">\(\operatorname{conv}(\mathcal{X}, \mathcal{Y})\)</span> via</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" title="1"><span class="kw">class</span> ConvexHull(RegretMinimizer):</a>
<a class="sourceLine" id="cb8-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, r_x: RegretMinimizer, r_y: RegretMinimizer):</a>
<a class="sourceLine" id="cb8-3" title="3">        <span class="va">self</span>.r_x, <span class="va">self</span>.r_y <span class="op">=</span> r_x, r_y</a>
<a class="sourceLine" id="cb8-4" title="4">        <span class="va">self</span>.r_simplex <span class="op">=</span> RegretMatcher(<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb8-5" title="5"></a>
<a class="sourceLine" id="cb8-6" title="6">    <span class="kw">def</span> next_strategy(<span class="va">self</span>) <span class="op">-&gt;</span> np.ndarray:</a>
<a class="sourceLine" id="cb8-7" title="7">        x_t <span class="op">=</span> <span class="va">self</span>.r_x.next_strategy()</a>
<a class="sourceLine" id="cb8-8" title="8">        y_t <span class="op">=</span> <span class="va">self</span>.r_y.next_strategy()</a>
<a class="sourceLine" id="cb8-9" title="9">        p1_t, p2_t <span class="op">=</span> <span class="va">self</span>.r_simplex.next_strategy()</a>
<a class="sourceLine" id="cb8-10" title="10">        <span class="cf">return</span> p1_t <span class="op">*</span> x_t <span class="op">+</span> p2_t <span class="op">*</span> y_t</a>
<a class="sourceLine" id="cb8-11" title="11"></a>
<a class="sourceLine" id="cb8-12" title="12">    <span class="kw">def</span> observe_utility(<span class="va">self</span>, utility_vector: np.ndarray):</a>
<a class="sourceLine" id="cb8-13" title="13">        <span class="va">self</span>.r_x.observe_utility(utility_vector)</a>
<a class="sourceLine" id="cb8-14" title="14">        <span class="va">self</span>.r_y.observe_utility(utility_vector)</a>
<a class="sourceLine" id="cb8-15" title="15">        utility_augmented_vec <span class="op">=</span> np.array([</a>
<a class="sourceLine" id="cb8-16" title="16">            np.dot(utility_vector, <span class="va">self</span>.r_x.current_strategy),</a>
<a class="sourceLine" id="cb8-17" title="17">            np.dot(utility_vector, <span class="va">self</span>.r_y.current_strategy)</a>
<a class="sourceLine" id="cb8-18" title="18">        ])</a>
<a class="sourceLine" id="cb8-19" title="19">        <span class="va">self</span>.r_simplex.observe_utility(utility_augmented_vec)</a></code></pre></div>
<p>How does the cumulative regret grow in this case? By definition,</p>
<p><span class="math display">\[
\begin{align*}
R^T &amp;= \max_{\hat{\lambda}\in\Delta^2, \hat{x}\in\mathcal{X},\hat{y}\in\mathcal{Y}}\left\{
            \sum_{t=1}^T\hat{\lambda}_1(\ell^+)^\top\hat{x}+\hat{\lambda}_2(\ell^+)^\top\hat{y}
        \right\} - \left(
            \sum_{t=1}^T\lambda_1^t(\ell^+)^\top x_t + \lambda_2^t(\ell^+)^\top y_t\right) \\
    &amp;= \max_{\hat{\lambda}\in\Delta^2}\left\{
        \hat{\lambda}_1\max_{\hat{x}\in\mathcal{X}}\left\{\sum_{t=1}^T(\ell^+)^\top\hat{x}\right\}
        + \hat{\lambda}_2\max_{\hat{y}\in\mathcal{Y}}\left\{\sum_{t=1}^T(\ell^+)^\top\hat{y}\right\}\right\}
        - \left(\sum_{t=1}^T\lambda_1^t(\ell^+)^\top x_t + \lambda_2^t(\ell^+)^\top y_t\right)
\end{align*}
\]</span></p>
<p>as all components <span class="math inline">\(\hat{\lambda}_1, \hat{\lambda}_2\)</span> are nonnegative. Also,</p>
<p><span class="math display">\[ \max_{\hat{x}\in\mathcal{X}}\left\{\sum_{t=1}^T(\ell^+)^\top\hat{x}\right\} = R^T_\mathcal{X} + \sum_{t=1}^T(\ell^+)^\top x_t \]</span></p>
<p>and similarly for the other inner term. So</p>
<p><span class="math display">\[ R^T = \max_{\hat{\lambda}\in\Delta^2}\left\{
    \left(\sum_{t=1}^T \hat{\lambda}_1(\ell^+)^\top x_t + \hat{\lambda}_2(\ell^+)^\top y_t\right) 
        + \hat{\lambda}_1 R^T_\mathcal{X} + \hat{\lambda}_2 R^T_\mathcal{Y} \right\}
        - \left(\sum_{t=1}^T\lambda^t_1(\ell^+)^\top x_t + \lambda^t_2(\ell^+)^\top y_t\right) \]</span></p>
<p>As for <span class="math inline">\((\hat{\lambda}_1, \hat{\lambda}_2)\in\Delta^2\)</span>, we have trivially</p>
<p><span class="math display">\[ \hat{\lambda}_1 R^T_\mathcal{X} + \hat{\lambda}_2 R^T_\mathcal{Y} \le \max\{R^T_\mathcal{X}, R^T_\mathcal{Y}\} \]</span></p>
<p>which implies</p>
<p><span class="math display">\[ R^T \le R^T_{\Delta} + \max\{R^T_\mathcal{X}, R^T_\mathcal{Y}\} \]</span></p>
<p>Hence if <span class="math inline">\(R^T_\mathcal{X}, R^T_\mathcal{Y}, R^T_{\Delta}\)</span> grow sublinearly, so does <span class="math inline">\(R^T\)</span>.</p>
<p>We close this section with the construction of a no-swap regret learning algorithm for the simplex <span class="math inline">\(\Delta^n\)</span>. In the literature, a lot of research is focused on creating external regret minimizers. However, in the previous section we gave a definition of <span class="math inline">\(\Phi\)</span>-regret minimization for general <span class="math inline">\(\Phi\)</span>. How can we construct no-<span class="math inline">\(\Phi\)</span> regret learners generically?</p>
<p>In 2008, a <a href="https://www.cs.cmu.edu/~ggordon/gordon-greenwald-marks-icml-phi-regret.pdf">paper</a> by Gordon et al. gives a way to construct a <span class="math inline">\(\Phi\)</span>-regret minimizer for <span class="math inline">\(\mathcal{X}\)</span> from a regret minimizer over the set of functions <span class="math inline">\(\phi\in\Phi\)</span>.</p>
<p><strong>Theorem</strong> (Gordon et al.): Let <span class="math inline">\(\mathcal{R}\)</span> be a <em>deterministic</em> regret minimizer over <span class="math inline">\(\Phi\)</span> with sublinear cumulative regret, and assume each <span class="math inline">\(\phi\in\Phi\)</span> has a fixed point, <span class="math inline">\(\phi(x)=x\in\mathcal{X}\)</span>. Then a <span class="math inline">\(\Phi\)</span>-regret minimizer <span class="math inline">\(\mathcal{R}_\Phi\)</span> can be constructed from <span class="math inline">\(\mathcal{R}\)</span> as:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" title="1"><span class="kw">class</span> PhiRegretLearner(RegretMinimizer):</a>
<a class="sourceLine" id="cb9-2" title="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, regret_learner: RegretMinimizer):</a>
<a class="sourceLine" id="cb9-3" title="3">        <span class="va">self</span>.regret_learner <span class="op">=</span> regret_learner</a>
<a class="sourceLine" id="cb9-4" title="4">        <span class="va">self</span>.last_fixpoint <span class="op">=</span> <span class="va">None</span></a>
<a class="sourceLine" id="cb9-5" title="5"></a>
<a class="sourceLine" id="cb9-6" title="6">    <span class="kw">def</span> next_strategy(<span class="va">self</span>) <span class="op">-&gt;</span> np.ndarray:</a>
<a class="sourceLine" id="cb9-7" title="7">        <span class="co"># since regret_learner is a regret minimizer over \Phi</span></a>
<a class="sourceLine" id="cb9-8" title="8">        phi_t <span class="op">=</span> <span class="va">self</span>.regret_learner.next_strategy()</a>
<a class="sourceLine" id="cb9-9" title="9">        <span class="co"># get fixed point</span></a>
<a class="sourceLine" id="cb9-10" title="10">        x_t <span class="op">=</span> fixpoint(phi_t)</a>
<a class="sourceLine" id="cb9-11" title="11">        <span class="va">self</span>.last_fixpoint <span class="op">=</span> x_t</a>
<a class="sourceLine" id="cb9-12" title="12">        <span class="cf">return</span> x_t</a>
<a class="sourceLine" id="cb9-13" title="13"></a>
<a class="sourceLine" id="cb9-14" title="14">    <span class="kw">def</span> observe_utility(<span class="va">self</span>, utility_vec: np.ndarray):</a>
<a class="sourceLine" id="cb9-15" title="15">        x_t <span class="op">=</span> <span class="va">self</span>.last_fixpoint</a>
<a class="sourceLine" id="cb9-16" title="16">        <span class="kw">def</span> _linear_utility_functional(phi):</a>
<a class="sourceLine" id="cb9-17" title="17">            <span class="cf">return</span> np.dot(utility_vec, phi(x_t))</a>
<a class="sourceLine" id="cb9-18" title="18">        <span class="va">self</span>.regret_learner.observe_utility(_linear_utility_functional)</a></code></pre></div>
<p>where we assume <code>fixpoint</code> is a function that can deterministically get a fixed point of the function <span class="math inline">\(phi\in\Phi\)</span>. Furthermore, <span class="math inline">\(R^T=R^T_\Phi\)</span>, so <span class="math inline">\(\mathcal{R}_\Phi\)</span> has sublinear regret.</p>
<p><em>Proof</em>: For a sequence <span class="math inline">\(\phi_1,\phi_2,...\)</span> output by <span class="math inline">\(\mathcal{R}\)</span> with utilities <span class="math inline">\(\phi\mapsto\langle\ell^1,\phi(x_1)\rangle,\phi\mapsto\langle\ell^2,\phi(x_2)\rangle,...\)</span> we have</p>
<p><span class="math display">\[ R^T=\max_{\hat{\phi}\in\Phi}\left\{\sum_{t=1}^T\left(\langle\ell^t,\hat{\phi}(x_t)\rangle-\langle\ell^t,\phi_t(x_t)\rangle\right)\right\} \]</span></p>
<p>As <span class="math inline">\(\phi_t(x_t)=x_t\)</span>, we get</p>
<p><span class="math display">\[ R^T = \max_{\hat{\phi}\in\Phi}\left\{\sum_{t=1}^T\left(\langle\ell^t,\hat{\phi}(x_t)\rangle-\langle\ell^t,x_t\rangle\right)\right\} \]</span></p>
<p>which is exactly <span class="math inline">\(R^T_\Phi\)</span>. <span class="math inline">\(\square\)</span>.</p>
<p>As an application of this theorem, we will construct a no-swap regret learner for <span class="math inline">\(\Delta^n\)</span>. Recall that swap regret learning is the same as <span class="math inline">\(\Phi^\text{all}\)</span>-regret minimization, where</p>
<p><span class="math display">\[ \Phi^\text{all} = \left\{\text{all linear functions }\Delta^n\to\Delta^n\right\} \]</span></p>
<p>for <span class="math inline">\(\Delta^n = \left\{(x_1,...,x_n)\in\mathbf{R}^n_{\ge 0} : x_1 +...+ x_n = 1\right\}\)</span>. Note that a linear map <span class="math inline">\(f:\mathbf{R}^n\to\mathbf{R}^n\)</span> restricts to a map <span class="math inline">\(\Delta^n\to\Delta^n\)</span> if it sends the basis vectors <span class="math inline">\(\{e_1,...,e_n\}\)</span> to <span class="math inline">\(\{v_1,...,v_n\}\subset\Delta^n\)</span>. But <span class="math inline">\(v_i\in\Delta^n\)</span> implies that the matrix <span class="math inline">\(M\)</span> formed by the <span class="math inline">\(v_i\)</span>’s concatenated together as column vectors is <strong>(column)-stochastic</strong>, i.e. columns sum to 1 and is nonnegative.</p>
<p>So in this case, <span class="math inline">\(f(x)=Mx\)</span> where <span class="math inline">\(M\)</span> is stochastic, so we can describe for the probability simplex case that</p>
<p><span class="math display">\[ \Phi^\text{all} = \left\{M\in\mathbf{R}^{n\times n}_{\ge 0}: M\text{ is column-stochastic}\right\} \]</span></p>
    </section>
</article>

        </main>

        <footer>
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </footer>
    </body>
</html>
