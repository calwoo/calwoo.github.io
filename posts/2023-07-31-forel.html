<!doctype html>
<html lang="en">
    <head>
        <link rel="icon" type="image/x-icon" href="../favicon.ico" />
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Online learning and FoReL - Calvin Woo's blog</title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../css/syntax.css" />
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG" crossorigin="anonymous">
    </head>
    <body>
        <header>
            <div class="logo">
                <a href="../">Calvin's Notebook</a>
            </div>
            <nav>
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../archive.html">Archive</a>
            </nav>
        </header>

        <main role="main">
            <h1>Online learning and FoReL</h1>
            <article>
    <section class="header">
        Posted on July 31, 2023
        
            by Calvin
        
    </section>
    <section>
        <p><strong>Note: This blog post is still a rough draft. Read on with caution.</strong></p>
<p>In part 2 of our unthemed dive into the reinforcement learning literature, we will be taking a look at online (convex) optimization and some reinforcement learning algorithms that came out of it, applied to imperfect-information zero-sum games.</p>
<p>The last post focused on counterfactual regret minimization, which was also an online algorithm for choosing the optimal strategies for an agent. The success of counterfactual regret minimization came from its strong theoretical guarantees of sublinear regret growth, along with its generality. As such, it seems fitting to start with a general overview of the ideas behind online optimization and see what other ideas came out of it that could be fruitful for future AIs.</p>
<h3 id="online-learning">online learning</h3>
<p>In machine learning, online learning is the process of continuously adapting and making decisions from streams of information: at each point in a time <span class="math inline">\(t\)</span>, an online learning algorithm is given an informational signal <span class="math inline">\(x_t\)</span> from a space <span class="math inline">\(\mathcal{X}\)</span>, and decides on an action <span class="math inline">\(a_t\in\mathcal{A}\)</span> to perform. After their decision, the environment/opponent chooses a loss function <span class="math inline">\(\ell^t\)</span> and causes the agent to suffer a loss <span class="math inline">\(\ell^t(x_t, a_t)\)</span>. The algorithm learns from this loss and updates its processes for the next time.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_timesteps):</a>
<a class="sourceLine" id="cb1-2" title="2">    signal_t <span class="op">=</span> env.receive_signal()</a>
<a class="sourceLine" id="cb1-3" title="3">    action_t <span class="op">=</span> learner.decide(signal_t)</a>
<a class="sourceLine" id="cb1-4" title="4">    loss_t <span class="op">=</span> env.receive_loss()</a>
<a class="sourceLine" id="cb1-5" title="5">    loss <span class="op">=</span> loss_t(signal_t, action_t)</a>
<a class="sourceLine" id="cb1-6" title="6">    learner.suffer(loss)</a></code></pre></div>
<p>The goal of the learner is to minimize their <strong>regret</strong></p>
<p><span class="math display">\[ R^T = \max_{a^*\in\mathcal{A}}\left\{\sum_{t=1}^T\ell^t(x_t, a^*)\right\} - \sum_{t=1}^T\ell^t(x_t, a_t) \]</span></p>
<p>We call such an online learning setting <strong>learnable</strong> if we can achieve sublinear regret in <span class="math inline">\(T\)</span>.</p>
<p>A special case that we will focus on is the setting of <strong>online convex optimization</strong>. Here, we receive <strong>no</strong> signals <span class="math inline">\(x_t\)</span> from the environment, and instead our “actions” will be points in a convex domain <span class="math inline">\(a_t\in\mathcal{K}\)</span>. The loss here will be given by an arbitrary convex function <span class="math inline">\(f_t\)</span>, and so the goal of our convex optimizer is to minimize the regret term</p>
<p><span class="math display">\[ R^T = \max_{a^*\in\mathcal{K}}\left\{\sum_{t=1}^T f_t(a^*)\right\} - \sum_{t=1}^T f_t(a_t) \]</span></p>
<p>Our goal in this post is to introduce and derive some important algorithms for solving the online convex optimization problem, and apply these algorithms to game-theoretic solutions in modern machine learning.</p>
<h3 id="convexity-primer">convexity primer</h3>
<p>First, we give a brief primer to notions of convexity in mathematics.</p>
    </section>
</article>

        </main>

        <footer>
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </footer>
    </body>
</html>
