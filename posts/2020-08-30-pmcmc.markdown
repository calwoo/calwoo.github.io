---
title: Particle MCMC methods
author: Calvin
---

**Note: This blog post is still a rough draft. Read on with caution.**


Often in machine learning we want to understand the dynamics of sequences of data, often ordered temporally. In this post we will talk about inference methods for Bayesian dynamic latent variable models, a type of probabilistic graphical model.


### importance sampling

In a nutshell, Bayesian inference of nonlinear state-space models are analytically intractible, hence the need for approximate methods. To set notation, we will deal with **latent Markov models**, in which sequences of latent states only depend conditionally on the previous state, not the full history of past states before it. We observe a sequence of **observations** $y_t$ for $t=1,...,T$ from the model-- this can be something like the total lab test volume in a given time-window, or the number of patients testing positive for an infectious disease (like COVID-19). The main assumption of a latent variable model is that the observations are generated by a set of underlying **latent variables** $x_t$, which are themselves time-varying. 

In the dynamic latent variable models we're studying, the dynamics in the model are given by the latent state **transitions** $p(x_t|x_{t-1}) = f(x_{t-1}, \theta_\text{trans})$ and **observation probabilities** $p(y_t|x_t) = g(x_t, \theta_\text{obs})$. Here the tunable parameters are denoted by $\theta=(\theta_\text{obs}, \theta_\text{trans})$.

Combined we get a full joint distribution

$$ p(x_{0:T}, y_{1:T}, \theta) = \prod_{i=1}^T p(y_t|x_t, \theta_\text{obs})\prod_{i=1}^T p(x_t|x_{t-1}, \theta_\text{trans}) p(x_0|\theta) p(\theta) $$

**The goal** here is to perform Bayesian inference over the latent variables; that is, we want to compute $p(x_{0:T}|y_{1:T})$. But as we said before, this is analytically intractible, so the best we can do is approximate it by *sampling*. However, sampling from $p(x_{0:T}|y_{1:T})$ is also hard!

Instead, we take a cue from Monte Carlo methods and sample instead from a **proposal** distribution $q(x_{0:T}|y_{1:T})$ with larger tails than the model distribution. **Importance sampling** corrects for this bias by scaling the samples from $q$ by an appropriate weight. That is, for a given integrable function $\zeta$, we compute our desired expectations as

$$ \begin{align} \mathbf{E}_{x\sim p(x_{0:T}|y_{1:T})}[\zeta(x)] &= \mathbf{E}_{x\sim q(x_{0:T}|y_{1:T})}\left[\frac{p(x_{0:T}|y_{1:T})}{q(x_{0:T}|y_{1:T})}\zeta(x)\right] \\ 
&= \mathbf{E}_{x\sim q(x_{0:T}|y_{1:T})}[\omega(x)\zeta(x)] \end{align}
$$

where $\omega(x) = \frac{p(x_{0:T}|y_{1:T})}{q(x_{0:T}|y_{1:T})}$. 

What's the problem here? As we said before, sampling from $p(x_{0:T}|y_{1:T})$ is also hard! Often, it is easier to sample from $p(x_{0:T}, y_{1:T})$, but then we would have to deal with the normalizing factor $p(y_{1:T})$. How do we deal with this?

The main trick here is to realize that the normalizing factor is itself an expectation, and hence can be computed via Monte Carlo itself. Indeed,

$$ p(y_{1:T}) = \int_x p(x_{0:T}, y_{1:T}) dx_{0:T} = \int_x \omega(x) q(x_{0:T}|y_{1:T}) dx_{0:T} $$

Putting this all together, we get the **self-normalized importance sampler**, described in (pseudo)code:

```python
def normalize(wgts):
    return wgts / sum(wgts)

def importance(model, proposal, n_samples):
    q_samples = proposal.sample(n_samples)
    weights = (model.log_prob(q_samples) - proposal.log_prob(q_samples)).exp()
    norm_wgts = normalize(weights)
    return norm_wgts, q_samples
```


### particle filter

The difficulty with importance sampling is choosing a good proposal distribution. If our goal is to compute the marginals $p(x_t|y_{1:t})$, then it turns out that we don't need to construct a fancy proposal. It turns out we can reuse the transition probabilities in our model to perform importance sampling. This is called the **particle filter** method.

The method is straightforward: we want to sample from $p(x_t|y_{1:t})$, so we perform this iteratively, starting with a prior $p(x_0)$. Now the trick is that our proposal is the transition probability $p(x_1|x_0)$, which we assume is known. So sampling $\widetilde{x}^{(i)}_1\sim p(x_1|x_0)$ for $i=1..N$ gives a batch of **particles** that we will use to represent our sampled distribution.

```python
def bootstrap(ys, n_particles=100):
    n_obs = len(ys)
    x = np.zeros(shape=(n_obs, n_particles))
    # initialize initial predicted states
    x[0] = x0_init(n_particles)

    # transition to x_1
    x[1] = transition(x[0])
```

Calculating weights we see that

$$ \widetilde{\omega}_1^{(i)} = \frac{p(\widetilde{x}^{(i)}_1|y_1)}{p(\widetilde{x}^{(i)}_1|x_0)} \propto p(y_1|\widetilde{x}^{(i)}_1) = g(\widetilde{x}^{(i)}_1, \theta_\text{obs}) $$

```python
    # generate y0 and importance weights
    importance_weights = obs_prob(ys[1], x[1])
    importance_weights = normalize(importance_weights)
```

so normalizing $\omega_1^{(i)} = \frac{\widetilde{\omega}_1^{(i)}}{\sum_{j=1}^N  \widetilde{\omega}_1^{(j)}}$ gives the importance weights to the sampled particles. That's the first trick to the particle filter. The second trick is noting that as time goes on, our weights *will collapse to zero*, and eventually only one particle survives. This is untenable, so we instead **resample** the particles according to their importance weights

$$ a_1^{(i)} \sim \text{Categorical}(\omega^{(j)}_1\text{ for }j=1...N) $$

where $a_1$ are the indices of the new resampled particles. 

```python
    # resample particles
    resampled_idx = random.choice(range(n_particles), size=n_particles, p=importance_weights)
    x[1] = x[1, resampled_idx]
```

Now we repeat this for each new time-step. We sample $\widetilde{x}^{(i)}_2\sim p(x_2|x^{(i)}_1)$ by *propagating* each particle, and compute importance weights as normal:

$$ \widetilde{\omega}_2^{(i)} = \frac{p(\widetilde{x}^{(i)}_2|y_2)}{p(\widetilde{x}^{(i)}_2|x^{(i)}_1)} \propto p(y_2|\widetilde{x}^{(i)}_2) = g(\widetilde{x}^{(i)}_2, \theta_\text{obs}) $$
 
We then normalize and resample.

```python
    for t in range(2, n_obs):
        # transition and compute weights
        x[t] = transition(x[t-1], p=p)
        importance_weights = normalize(obs_prob(ys[t], x[t]))
        # resample particles
        resampled_idx = np.random.choice(range(n_particles), size=n_particles, p=importance_weights)
        x[t] = x[t, resampled_idx]
        
    return x
```

This is the essence behind a particle filter! 



