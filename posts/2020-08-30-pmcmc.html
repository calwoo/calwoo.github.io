<!doctype html>
<html lang="en">
    <head>
        <link rel="icon" type="image/x-icon" href="../favicon.ico" />
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Particle MCMC methods - Calvin Woo's blog</title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../css/syntax.css" />
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css" integrity="sha384-BdGj8xC2eZkQaxoQ8nSLefg4AV4/AwB3Fj+8SUSo7pnKP6Eoy18liIKTPn9oBYNG" crossorigin="anonymous">
    </head>
    <body>
        <header>
            <div class="logo">
                <a href="../">Calvin's Notebook</a>
            </div>
            <nav>
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../archive.html">Archive</a>
            </nav>
        </header>

        <main role="main">
            <h1>Particle MCMC methods</h1>
            <article>
    <section class="header">
        Posted on August 30, 2020
        
            by Calvin
        
    </section>
    <section>
        <p><strong>Note: This blog post is still a rough draft. Read on with caution.</strong></p>
<p>Often in machine learning we want to understand the dynamics of sequences of data, often ordered temporally. In this post we will talk about inference methods for Bayesian dynamic latent variable models, a type of probabilistic graphical model.</p>
<h3 id="importance-sampling">importance sampling</h3>
<p>In a nutshell, Bayesian inference of nonlinear state-space models are analytically intractible, hence the need for approximate methods. To set notation, we will deal with <strong>latent Markov models</strong>, in which sequences of latent states only depend conditionally on the previous state, not the full history of past states before it. We observe a sequence of <strong>observations</strong> <span class="math inline">\(y_t\)</span> for <span class="math inline">\(t=1,...,T\)</span> from the model– this can be something like the total lab test volume in a given time-window, or the number of patients testing positive for an infectious disease (like COVID-19). The main assumption of a latent variable model is that the observations are generated by a set of underlying <strong>latent variables</strong> <span class="math inline">\(x_t\)</span>, which are themselves time-varying.</p>
<p>In the dynamic latent variable models we’re studying, the dynamics in the model are given by the latent state <strong>transitions</strong> <span class="math inline">\(p(x_t|x_{t-1}) = f(x_{t-1}, \theta_\text{trans})\)</span> and <strong>observation probabilities</strong> <span class="math inline">\(p(y_t|x_t) = g(x_t, \theta_\text{obs})\)</span>. Here the tunable parameters are denoted by <span class="math inline">\(\theta=(\theta_\text{obs}, \theta_\text{trans})\)</span>.</p>
<p>Combined we get a full joint distribution</p>
<p><span class="math display">\[ p(x_{0:T}, y_{1:T}, \theta) = \prod_{i=1}^T p(y_t|x_t, \theta_\text{obs})\prod_{i=1}^T p(x_t|x_{t-1}, \theta_\text{trans}) p(x_0|\theta) p(\theta) \]</span></p>
<p><strong>The goal</strong> here is to perform Bayesian inference over the latent variables; that is, we want to compute <span class="math inline">\(p(x_{0:T}|y_{1:T})\)</span>. But as we said before, this is analytically intractible, so the best we can do is approximate it by <em>sampling</em>. However, sampling from <span class="math inline">\(p(x_{0:T}|y_{1:T})\)</span> is also hard!</p>
<p>Instead, we take a cue from Monte Carlo methods and sample instead from a <strong>proposal</strong> distribution <span class="math inline">\(q(x_{0:T}|y_{1:T})\)</span> with larger tails than the model distribution. <strong>Importance sampling</strong> corrects for this bias by scaling the samples from <span class="math inline">\(q\)</span> by an appropriate weight. That is, for a given integrable function <span class="math inline">\(\zeta\)</span>, we compute our desired expectations as</p>
<p><span class="math display">\[ \begin{align} \mathbf{E}_{x\sim p(x_{0:T}|y_{1:T})}[\zeta(x)] &amp;= \mathbf{E}_{x\sim q(x_{0:T}|y_{1:T})}\left[\frac{p(x_{0:T}|y_{1:T})}{q(x_{0:T}|y_{1:T})}\zeta(x)\right] \\ 
&amp;= \mathbf{E}_{x\sim q(x_{0:T}|y_{1:T})}[\omega(x)\zeta(x)] \end{align}
\]</span></p>
<p>where <span class="math inline">\(\omega(x) = \frac{p(x_{0:T}|y_{1:T})}{q(x_{0:T}|y_{1:T})}\)</span>.</p>
<p>What’s the problem here? As we said before, sampling from <span class="math inline">\(p(x_{0:T}|y_{1:T})\)</span> is also hard! Often, it is easier to sample from <span class="math inline">\(p(x_{0:T}, y_{1:T})\)</span>, but then we would have to deal with the normalizing factor <span class="math inline">\(p(y_{1:T})\)</span>. How do we deal with this?</p>
<p>The main trick here is to realize that the normalizing factor is itself an expectation, and hence can be computed via Monte Carlo itself. Indeed,</p>
<p><span class="math display">\[ p(y_{1:T}) = \int_x p(x_{0:T}, y_{1:T}) dx_{0:T} = \int_x \omega(x) q(x_{0:T}|y_{1:T}) dx_{0:T} \]</span></p>
<p>Putting this all together, we get the <strong>self-normalized importance sampler</strong>, described in (pseudo)code:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"><span class="kw">def</span> normalize(wgts):</a>
<a class="sourceLine" id="cb1-2" title="2">    <span class="cf">return</span> wgts <span class="op">/</span> <span class="bu">sum</span>(wgts)</a>
<a class="sourceLine" id="cb1-3" title="3"></a>
<a class="sourceLine" id="cb1-4" title="4"><span class="kw">def</span> importance(model, proposal, n_samples):</a>
<a class="sourceLine" id="cb1-5" title="5">    q_samples <span class="op">=</span> proposal.sample(n_samples)</a>
<a class="sourceLine" id="cb1-6" title="6">    weights <span class="op">=</span> (model.log_prob(q_samples) <span class="op">-</span> proposal.log_prob(q_samples)).exp()</a>
<a class="sourceLine" id="cb1-7" title="7">    norm_wgts <span class="op">=</span> normalize(weights)</a>
<a class="sourceLine" id="cb1-8" title="8">    <span class="cf">return</span> norm_wgts, q_samples</a></code></pre></div>
<h3 id="particle-filter">particle filter</h3>
<p>The difficulty with importance sampling is choosing a good proposal distribution. If our goal is to compute the marginals <span class="math inline">\(p(x_t|y_{1:t})\)</span>, then it turns out that we don’t need to construct a fancy proposal. It turns out we can reuse the transition probabilities in our model to perform importance sampling. This is called the <strong>particle filter</strong> method.</p>
<p>The method is straightforward: we want to sample from <span class="math inline">\(p(x_t|y_{1:t})\)</span>, so we perform this iteratively, starting with a prior <span class="math inline">\(p(x_0)\)</span>. Now the trick is that our proposal is the transition probability <span class="math inline">\(p(x_1|x_0)\)</span>, which we assume is known. So sampling <span class="math inline">\(\widetilde{x}^{(i)}_1\sim p(x_1|x_0)\)</span> for <span class="math inline">\(i=1..N\)</span> gives a batch of <strong>particles</strong> that we will use to represent our sampled distribution.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="kw">def</span> bootstrap(ys, n_particles<span class="op">=</span><span class="dv">100</span>):</a>
<a class="sourceLine" id="cb2-2" title="2">    n_obs <span class="op">=</span> <span class="bu">len</span>(ys)</a>
<a class="sourceLine" id="cb2-3" title="3">    x <span class="op">=</span> np.zeros(shape<span class="op">=</span>(n_obs, n_particles))</a>
<a class="sourceLine" id="cb2-4" title="4">    <span class="co"># initialize initial predicted states</span></a>
<a class="sourceLine" id="cb2-5" title="5">    x[<span class="dv">0</span>] <span class="op">=</span> x0_init(n_particles)</a>
<a class="sourceLine" id="cb2-6" title="6"></a>
<a class="sourceLine" id="cb2-7" title="7">    <span class="co"># transition to x_1</span></a>
<a class="sourceLine" id="cb2-8" title="8">    x[<span class="dv">1</span>] <span class="op">=</span> transition(x[<span class="dv">0</span>])</a></code></pre></div>
<p>Calculating weights we see that</p>
<p><span class="math display">\[ \widetilde{\omega}_1^{(i)} = \frac{p(\widetilde{x}^{(i)}_1|y_1)}{p(\widetilde{x}^{(i)}_1|x_0)} \propto p(y_1|\widetilde{x}^{(i)}_1) = g(\widetilde{x}^{(i)}_1, \theta_\text{obs}) \]</span></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1">    <span class="co"># generate y0 and importance weights</span></a>
<a class="sourceLine" id="cb3-2" title="2">    importance_weights <span class="op">=</span> obs_prob(ys[<span class="dv">1</span>], x[<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb3-3" title="3">    importance_weights <span class="op">=</span> normalize(importance_weights)</a></code></pre></div>
<p>so normalizing <span class="math inline">\(\omega_1^{(i)} = \frac{\widetilde{\omega}_1^{(i)}}{\sum_{j=1}^N \widetilde{\omega}_1^{(j)}}\)</span> gives the importance weights to the sampled particles. That’s the first trick to the particle filter. The second trick is noting that as time goes on, our weights <em>will collapse to zero</em>, and eventually only one particle survives. This is untenable, so we instead <strong>resample</strong> the particles according to their importance weights</p>
<p><span class="math display">\[ a_1^{(i)} \sim \text{Categorical}(\omega^{(j)}_1\text{ for }j=1...N) \]</span></p>
<p>where <span class="math inline">\(a_1\)</span> are the indices of the new resampled particles.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1">    <span class="co"># resample particles</span></a>
<a class="sourceLine" id="cb4-2" title="2">    resampled_idx <span class="op">=</span> random.choice(<span class="bu">range</span>(n_particles), </a>
<a class="sourceLine" id="cb4-3" title="3">                                  size<span class="op">=</span>n_particles, p<span class="op">=</span>importance_weights)</a>
<a class="sourceLine" id="cb4-4" title="4">    x[<span class="dv">1</span>] <span class="op">=</span> x[<span class="dv">1</span>, resampled_idx]</a></code></pre></div>
<p>Now we repeat this for each new time-step. We sample <span class="math inline">\(\widetilde{x}^{(i)}_2\sim p(x_2|x^{(i)}_1)\)</span> by <em>propagating</em> each particle, and compute importance weights as normal:</p>
<p><span class="math display">\[ \widetilde{\omega}_2^{(i)} = \frac{p(\widetilde{x}^{(i)}_2|y_2)}{p(\widetilde{x}^{(i)}_2|x^{(i)}_1)} \propto p(y_2|\widetilde{x}^{(i)}_2) = g(\widetilde{x}^{(i)}_2, \theta_\text{obs}) \]</span></p>
<p>We then normalize and resample.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1">    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, n_obs):</a>
<a class="sourceLine" id="cb5-2" title="2">        <span class="co"># transition and compute weights</span></a>
<a class="sourceLine" id="cb5-3" title="3">        x[t] <span class="op">=</span> transition(x[t<span class="dv">-1</span>], p<span class="op">=</span>p)</a>
<a class="sourceLine" id="cb5-4" title="4">        importance_weights <span class="op">=</span> normalize(obs_prob(ys[t], x[t]))</a>
<a class="sourceLine" id="cb5-5" title="5">        <span class="co"># resample particles</span></a>
<a class="sourceLine" id="cb5-6" title="6">        resampled_idx <span class="op">=</span> random.choice(<span class="bu">range</span>(n_particles), </a>
<a class="sourceLine" id="cb5-7" title="7">                                      size<span class="op">=</span>n_particles, p<span class="op">=</span>importance_weights)</a>
<a class="sourceLine" id="cb5-8" title="8">        x[t] <span class="op">=</span> x[t, resampled_idx]</a>
<a class="sourceLine" id="cb5-9" title="9">        </a>
<a class="sourceLine" id="cb5-10" title="10">    <span class="cf">return</span> x</a></code></pre></div>
<p>This is the essence behind a particle filter! We will use this to provide the approximate likelihoods for more advanced particle MCMC methods, such as the particle Metropolis-Hastings algorithm.</p>
<h3 id="metropolis-hastings-and-mcmc">metropolis-hastings and mcmc</h3>
<p>Given a state space model above, ultimately our goal is to perform inference over latent states <strong>and</strong> parameters, <span class="math inline">\(p(x_{0:T}, \theta\mid y_{1:T})\)</span>. In the above particle filter method, we developed a way to approximate the marginal <span class="math inline">\(p(x_{0:T}\mid\theta, y_{1:T})\)</span>. So how can we compute the other marginal, the parameter inference <span class="math inline">\(p(\theta\mid y_{1:T})\)</span>? We can appeal to a standard MCMC method.</p>
<p><strong>Note:</strong> Why not importance sampling? Probably because finding an appropriate proposal distribution <span class="math inline">\(q(\theta\mid y_{1:T})\)</span> that has thicker tails than <span class="math inline">\(p(\theta\mid y_{1:T})\)</span> is hard to determine and build.</p>
<p>With that out of the way, let’s describe a basic MCMC method– the <strong>Metropolis-Hastings</strong> algorithm. The central idea of MCMC methods is to construct a <em>Markov chain</em> whose stationary distribution is the posterior distribution we want to sample from. That is, we want to perform a random walk in latent/parameter space such that after some time passes, the places visited by the walk will start to look like samples distributed according to the posterior.</p>
<p>Formally, MCMC seeks to construct a <strong>transition kernel</strong> <span class="math inline">\(P_\text{MCMC}(x\to y)\)</span>, which describes the Markovian walk from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span> in parameter space. Such kernel has a consistency equation, given by Chapman-Kolmogorov:</p>
<p><span class="math display">\[ P^{(2)}_\text{MCMC}(x\to z) = \int_y P_\text{MCMC}(x\to y) P_\text{MCMC}(y\to z) dy \]</span></p>
<p>where the <span class="math inline">\((2)\)</span> denotes the 2-step transition kernel.</p>
<p>By definition, a <strong>stationary distribution</strong> for a Markov kernel is a distribution <span class="math inline">\(\pi(x)\)</span> over the parameter space that is “fixed” by the kernel– that is,</p>
<p><span class="math display">\[ \pi(z) = \int_y \pi(y) P_\text{MCMC}(y\to z) dy \]</span></p>
<p>We see from the Chapman-Kolmogorov equation that <span class="math inline">\(P_\text{MCMC}(x\to -)\)</span> for any <span class="math inline">\(x\)</span> would be a close candidate, except that the step-size on each side is different. Under certain mathematical conditions (ergodicity), we can take limits, and show that for any initial point <span class="math inline">\(x\)</span>, the stationary distribution can be described as</p>
<p><span class="math display">\[ \pi(z) = \lim_{n\to\infty} P_\text{MCMC}^{(n)}(x\to z) \]</span></p>
<p>This shows the main point of MCMC– starting from any initial point, a random walk described by the kernel <span class="math inline">\(P_\text{MCMC}(x\to y)\)</span> eventually gives samples distributed according to the stationary distribution <span class="math inline">\(\pi(x)\)</span>.</p>
<p>However, this is a fairly difficult condition to check, as often constructing <span class="math inline">\(P_\text{MCMC}(x\to z)\)</span> shows that it’s hard to get a grasp on its limiting properties. An easier condition to check is that of <strong>time-reversibility</strong>: <span class="math inline">\(\pi(x) P_\text{MCMC}(x\to y) = \pi(y) P_\text{MCMC}(y\to x)\)</span>. A distribution <span class="math inline">\(\pi\)</span> that satisfies this condition is thus the stationary distribution of the MCMC kernel.</p>
<p><strong>Proof:</strong> This is an integral check, given by</p>
<p><span class="math display">\[ \begin{align} \int_x \pi(x) P_\text{MCMC}(x\to y) dx &amp;= \int_x \pi(y) P_\text{MCMC}(y\to x) dx \\
    &amp;= \pi(y) \end{align} \]</span></p>
<p>Now, we describe Metropolis-Hastings. Remember, in this setting our goal is to sample from <span class="math inline">\(p(\theta\mid y_{1:T})\)</span>, so this is our stationary distribution. To define the MH kernel, we first start with a <strong>proposal distribution</strong> <span class="math inline">\(q(x\to y)\)</span>. If <span class="math inline">\(q\)</span> satisfies time-reversibility, then we are done, and <span class="math inline">\(q(x\to y)\)</span> is our desired kernel. Most likely, this is not the case.</p>
<p>The trick of Metropolis-Hastings is to introduce an accept-reject step in the walk to correct for the bias. Let <span class="math inline">\(\alpha(x\to y)\)</span> be an <strong>acceptance probability</strong> of moving from <span class="math inline">\(x\)</span> to <span class="math inline">\(y\)</span>. We set our MH kernel to be <span class="math inline">\(P_\text{MH}(x\to y) = q(x\to y)\alpha(x\to y)\)</span>.</p>
<p>What is <span class="math inline">\(\alpha(x\to y)\)</span>? We choose this probability so that time-reversibility is satisfied: Under the assumption that <span class="math inline">\(\alpha(x\to y) &lt; 1\)</span>,</p>
<p><span class="math display">\[ \begin{align} \pi(x) P_\text{MH}(x\to y) &amp;= \pi(x) q(x\to y) \alpha(x\to y) \\
    &amp;= \pi(y) q(y\to x) \alpha(y\to x) \\
    &amp;= \pi(y) q(y\to x) \end{align} \]</span></p>
<p>where <span class="math inline">\(\alpha(y\to x) = 1\)</span>. This implies ultimately that</p>
<p><span class="math display">\[ \alpha(x\to y) = \min\left\{1, \frac{\pi(y)q(y\to x)}{\pi(x)q(x\to y)}\right\} \]</span></p>
<p>This fully describes the Metropolis-Hastings algorithm. In (pseudo)code, it looks like:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" title="1"><span class="kw">def</span> mh(likelihood, proposal, x_0, n_samples<span class="op">=</span><span class="dv">1000</span>):</a>
<a class="sourceLine" id="cb6-2" title="2">    x <span class="op">=</span> np.zeros(n_samples)</a>
<a class="sourceLine" id="cb6-3" title="3">    <span class="co"># initialize random walk</span></a>
<a class="sourceLine" id="cb6-4" title="4">    x[<span class="dv">0</span>] <span class="op">=</span> x_0</a>
<a class="sourceLine" id="cb6-5" title="5"></a>
<a class="sourceLine" id="cb6-6" title="6">    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n_samples):</a>
<a class="sourceLine" id="cb6-7" title="7">        <span class="co"># sample proposal</span></a>
<a class="sourceLine" id="cb6-8" title="8">        p_sample <span class="op">=</span> proposal(x[t<span class="dv">-1</span>]).sample()</a>
<a class="sourceLine" id="cb6-9" title="9">    </a>
<a class="sourceLine" id="cb6-10" title="10">        <span class="co"># coin flip</span></a>
<a class="sourceLine" id="cb6-11" title="11">        prob_y_to_x <span class="op">=</span> likelihood(p_sample) <span class="op">*</span> proposal(p_sample).log_prob(x[t<span class="dv">-1</span>]).exp()</a>
<a class="sourceLine" id="cb6-12" title="12">        prob_x_to_y <span class="op">=</span> likelihood(x[t<span class="dv">-1</span>]) <span class="op">*</span> proposal(x[t<span class="dv">-1</span>]).log_prob(p_sample).exp()</a>
<a class="sourceLine" id="cb6-13" title="13">        accept_prob <span class="op">=</span> <span class="bu">min</span>(<span class="dv">1</span>, prob_y_to_x <span class="op">/</span> prob_x_to_y)</a>
<a class="sourceLine" id="cb6-14" title="14"></a>
<a class="sourceLine" id="cb6-15" title="15">        <span class="cf">if</span> random.rand() <span class="op">&lt;</span> accept_prob:</a>
<a class="sourceLine" id="cb6-16" title="16">            x[t] <span class="op">=</span> p_sample</a>
<a class="sourceLine" id="cb6-17" title="17">        <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb6-18" title="18">            x[t] <span class="op">=</span> x[t<span class="dv">-1</span>]</a>
<a class="sourceLine" id="cb6-19" title="19">    </a>
<a class="sourceLine" id="cb6-20" title="20">    <span class="cf">return</span> x</a></code></pre></div>
<p>The key remark to make here is that the only place where the target distribution <span class="math inline">\(\pi(x)\)</span> appears is in the acceptance probability, where only a ratio appears. Hence, one upshot of the MH algorithm is that <span class="math inline">\(\pi(x)\)</span> doesn’t have to be <em>normalized</em>. This is a boon for Bayesian inference, where <span class="math inline">\(\pi(x)\)</span> can just be an unnormalized posterior, i.e. the likelihood and prior.</p>
<h3 id="auxiliary-variables">auxiliary variables</h3>
<p>However, what if the likelihood itself is hard to compute? In our situation we are trying to compute <span class="math inline">\(p(\theta\mid y_{1:T})\)</span> via Bayesian inference, so in the MH acceptance probability above we take as our <span class="math inline">\(\pi(\theta) = p(y_{1:T}\mid\theta)p(\theta)\)</span>. But we don’t know how to compute</p>
<p><span class="math display">\[ p(y_{1:T}\mid\theta) = \int_{x_{0:T}} p(y_{1:T}\mid x_{0:T},\theta) p(x_{0:T}\mid\theta) dx_{0:T} \]</span></p>
<p>exactly. Instead, suppose that we have an approximation to the likelihood <span class="math inline">\(\widehat{z}\simeq p(y_{1:T}\mid\theta)\)</span>. What we will do is treat this as a random variable <span class="math inline">\(\widehat{z}\sim \psi(z\mid\theta, y_{1:T})\)</span> and append this as an <strong>auxiliary variable</strong> to the joint distribution. Our joint then becomes</p>
<p><span class="math display">\[ \begin{align} \psi(\theta, z\mid y_{1:T}) &amp;= p(\theta\mid y_{1:T})\psi(z\mid\theta, y_{1:T}) \\
    &amp;\propto p(y_{1:T}\mid\theta)p(\theta)\psi(z\mid\theta, y_{1:T}) \\
    &amp;\simeq \widehat{z} p(\theta)\psi(z\mid\theta, y_{1:T}) \end{align} \]</span></p>
<p>where we used our approximation to the likelihood <span class="math inline">\(\widehat{z}\)</span>. Here, the final term is our target distribution <span class="math inline">\(\pi(\theta, z\mid y_{1:T}) = \widehat{z} p(\theta)\psi(z\mid\theta, y_{1:T})\)</span>. This looks different from our real target <span class="math inline">\(p(\theta\mid y_{1:T})\)</span>, not to mention that we have an extra auxiliary variable <span class="math inline">\(z\)</span>.</p>
<p>But it doesn’t matter! All that matters for <span class="math inline">\(\pi(\theta, z\mid y_{1:T})\)</span> is that its marginal for <span class="math inline">\(\theta\)</span> matches <span class="math inline">\(p(\theta\mid y_{1:T})\)</span>. In particular, we see that <span class="math inline">\(\widehat{z}\simeq p(y_{1:T}\mid\theta)\)</span> must be an <strong>unbiased</strong> estimator.</p>
<p>From this, we have all the ingredients we need to perform Metropolis-Hastings. At each step of the random walk, we sample <em>both</em> <span class="math inline">\(\theta'\)</span> and <span class="math inline">\(z'\)</span>, and accept-or-reject based on the acceptance probability (which one can compute):</p>
<p><span class="math display">\[ \alpha_\text{MH}(\theta', z') = \min\left\{1, \frac{z' p(\theta') q(\theta'\to\theta_\text{prev})}{z_\text{prev} p(\theta_\text{prev}) q(\theta_\text{prev}\to\theta')}\right\} \]</span></p>
<p>Finally, the projection to the <span class="math inline">\(\theta\)</span>-walk gives the resulting MCMC samples.</p>
<h3 id="particle-metropolis-hastings">particle metropolis-hastings</h3>
<p>Okay, all that sounds good, but– what are we taking as our approximation <span class="math inline">\(\widehat{z}\simeq p(y_{1:T}\mid\theta)\)</span>? As an integral form, the likelihood we want is</p>
<p><span class="math display">\[ p(y_{1:T}\mid\theta) = \int_{x_{0:T}} p(y_{1:T}\mid x_{0:T},\theta) p(x_{0:T}\mid\theta) dx_{0:T} \]</span></p>
<p>An idea is to approximate this using a <strong>Monte Carlo estimate</strong>. Factorizing the integral even further,</p>
<p><span class="math display">\[ p(y_{1:T}\mid\theta) = \prod_{i=1}^T \int_x p(y_t\mid x_t,\theta) p(x_t\mid\ y_{1:t-1}, \theta) dx_t \]</span></p>
<p>we can perform the MC estimation sequentially, and then multiply the estimates together. Exploiting the sequential nature of the model often decreases the variance of the final estimator.</p>
<p>But note that we already have an estimator for <span class="math inline">\(p(x_t\mid\ y_{1:t-1}, \theta)\)</span>– a <strong>particle filter</strong>! And in this situation, <span class="math inline">\(\omega_t\simeq p(y_t\mid x_t, \theta)\)</span> are the importance weights in the particle filter, so can ultimately produce likelihood estimators <span class="math inline">\(\widehat{z}\)</span> by performing a full run of a particle filter over the model and letting</p>
<p><span class="math display">\[ \widehat{z} = \prod_{t=1}^T \left[\frac{1}{N}\sum^N_{i=1} \omega^{(i)}_t\right] \]</span></p>
<p>Since the particle filter satisfies convergence guarentees, we see that <span class="math inline">\(\widehat{z}\)</span> is indeed an unbiased estimator of <span class="math inline">\(p(y_{1:T}\mid\theta)\)</span>. Combining this particle filter likelihood estimator with the Metropolis-Hastings method above, we get the <strong>particle Metropolis-Hastings algorithm</strong> for Bayesian parameter inference. In (pseudo)code this looks like:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" title="1"><span class="kw">def</span> pmh(pf, obs, prior, proposal, x_0, n_samples<span class="op">=</span><span class="dv">1000</span>, n_particles<span class="op">=</span><span class="dv">100</span>):</a>
<a class="sourceLine" id="cb7-2" title="2">    x <span class="op">=</span> np.zeros(n_samples)</a>
<a class="sourceLine" id="cb7-3" title="3">    z <span class="op">=</span> np.zeros(n_samples)</a>
<a class="sourceLine" id="cb7-4" title="4">    <span class="co"># initialize random walk</span></a>
<a class="sourceLine" id="cb7-5" title="5">    x[<span class="dv">0</span>] <span class="op">=</span> x_0</a>
<a class="sourceLine" id="cb7-6" title="6">    <span class="co"># particle filter returns particles and weights for each time step</span></a>
<a class="sourceLine" id="cb7-7" title="7">    particles, weights <span class="op">=</span> pf(obs, theta<span class="op">=</span>x[<span class="dv">0</span>]).run(n_particles)</a>
<a class="sourceLine" id="cb7-8" title="8">    z[<span class="dv">0</span>] <span class="op">=</span> prod(mean(weights, dim<span class="op">=</span><span class="dv">0</span>))</a>
<a class="sourceLine" id="cb7-9" title="9"></a>
<a class="sourceLine" id="cb7-10" title="10">    <span class="cf">for</span> m <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(obs)):</a>
<a class="sourceLine" id="cb7-11" title="11">        <span class="co"># sample proposal</span></a>
<a class="sourceLine" id="cb7-12" title="12">        p_sample <span class="op">=</span> proposal(x[m<span class="dv">-1</span>]).sample()</a>
<a class="sourceLine" id="cb7-13" title="13">        particles, weights <span class="op">=</span> pf(obs, theta<span class="op">=</span>p_sample).run(n_particles)</a>
<a class="sourceLine" id="cb7-14" title="14">        z_sample <span class="op">=</span> prod(mean(weights, dim<span class="op">=</span><span class="dv">0</span>))</a>
<a class="sourceLine" id="cb7-15" title="15"></a>
<a class="sourceLine" id="cb7-16" title="16">        <span class="co"># coin flip</span></a>
<a class="sourceLine" id="cb7-17" title="17">        prob_y_to_x <span class="op">=</span> z_sample <span class="op">*</span> (prior.log_prob(p_sample) <span class="op">+</span> </a>
<a class="sourceLine" id="cb7-18" title="18">                                  proposal(p_sample).log_prob(x[t<span class="dv">-1</span>])).exp()</a>
<a class="sourceLine" id="cb7-19" title="19">        prob_x_to_y <span class="op">=</span> z[t<span class="dv">-1</span>] <span class="op">*</span> (prior.log_prob(x[t<span class="dv">-1</span>]) <span class="op">+</span> </a>
<a class="sourceLine" id="cb7-20" title="20">                                proposal(x[t<span class="dv">-1</span>]).log_prob(p_sample)).exp()</a>
<a class="sourceLine" id="cb7-21" title="21">        accept_prob <span class="op">=</span> <span class="bu">min</span>(<span class="dv">1</span>, prob_y_to_x <span class="op">/</span> prob_x_to_y)</a>
<a class="sourceLine" id="cb7-22" title="22"></a>
<a class="sourceLine" id="cb7-23" title="23">        <span class="cf">if</span> random.rand() <span class="op">&lt;</span> accept_prob:</a>
<a class="sourceLine" id="cb7-24" title="24">            x[t] <span class="op">=</span> p_sample</a>
<a class="sourceLine" id="cb7-25" title="25">            z[t] <span class="op">=</span> z_sample</a>
<a class="sourceLine" id="cb7-26" title="26">        <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb7-27" title="27">            x[t] <span class="op">=</span> x[t<span class="dv">-1</span>]</a>
<a class="sourceLine" id="cb7-28" title="28">            z[t] <span class="op">=</span> z[t<span class="dv">-1</span>]</a>
<a class="sourceLine" id="cb7-29" title="29">    </a>
<a class="sourceLine" id="cb7-30" title="30">    <span class="co"># project out z</span></a>
<a class="sourceLine" id="cb7-31" title="31">    <span class="cf">return</span> x</a></code></pre></div>
<h3 id="probabilistic-programming">probabilistic programming</h3>
<p>If you think about it, a generative story for a joint distribution can be unraveled into a sequential model of states, and hence a version of particle MCMC methods apply here as well. In this way, the states <span class="math inline">\(x_t\)</span> refer to the total <strong>execution state</strong> of the program at that point in time, and observations <span class="math inline">\(y_t\)</span> correspond to the <code>observe</code> primitives in the PPL (i.e. conditioning statements).</p>
    </section>
</article>

        </main>

        <footer>
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </footer>
    </body>
</html>
